{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputData():\n",
    "    df = pd.read_csv(\"./data/q2/q2test.csv\")\n",
    "    X_0 = np.ones((len(df),1))\n",
    "    X1X2Y = df.to_numpy()\n",
    "    X0X1X2Y = np.append(X_0,X1X2Y, axis=1)\n",
    "    np.random.shuffle(X0X1X2Y)  #shuffling data to make it random for better distribution\n",
    "    return X0X1X2Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData():\n",
    "    Stheta = np.reshape(np.array([3,1,2]),(3,1))\n",
    "    SX_0 = np.ones((1000000))\n",
    "    SX_1 = np.random.normal(size = 1000000, loc = 3, scale = 2) # loc = mean scale = standard deviation\n",
    "    SX_2 = np.random.normal(size = 1000000, loc = -1, scale = 2)\n",
    "    SX = np.reshape(np.array([SX_0,SX_1,SX_2]).T,(-1,3) ) # creating design matrix of sample data\n",
    "    hypoT = np.dot(SX,Stheta)\n",
    "#     print(\"hypoThesis shape\")\n",
    "#     print(hypoT.shape)\n",
    "    SY = np.reshape(np.random.normal(size = 1000000, loc = hypoT.T , scale = math.sqrt(2)),(-1,1))\n",
    "    SX0X1X2Y = np.append(SX,SY,axis=1)\n",
    "#     print(SX0X1X2Y)\n",
    "    np.random.shuffle(SX0X1X2Y)\n",
    "#     print(SX0X1X2Y)\n",
    "#     plot to visualize the generated sample\n",
    "#     plt.scatter(SX0X1X2Y[:,1:2],SX0X1X2Y[:,3:4])\n",
    "#     plt.scatter(SX0X1X2Y[:,2:3],SX0X1X2Y[:,3:4])\n",
    "#     plt.show()\n",
    "\n",
    "    return SX0X1X2Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFuction(SX0X1X2Y,theta):\n",
    "    hypothesis = np.dot(SX0X1X2Y[:,0:3],np.array(theta))\n",
    "    hypothesis = np.reshape(hypothesis,(-1,1))\n",
    "    error = SX0X1X2Y[:,3:4]-hypothesis\n",
    "    errorSquared =error**2\n",
    "    examples = SX0X1X2Y.shape[0] #X.shape[0] represents number of training example\n",
    "    return np.sum(errorSquared)/(2*examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = sampleData()\n",
    "# print(a)\n",
    "# costFuction(a,[3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFunction(SX0X1X2Y,theta):\n",
    "    error = np.reshape(np.reshape(np.dot(SX0X1X2Y[:,0:3],np.array(theta)),(-1,1)) - SX0X1X2Y[:,3:4],(-1,1))  # h(X) -y\n",
    "    grad_cost = np.zeros((3,1))\n",
    "    grad_cost[0] = np.sum(error*SX0X1X2Y[:,0:1])/(SX0X1X2Y.shape[0])\n",
    "    grad_cost[1] = np.sum(error*SX0X1X2Y[:,1:2])/(SX0X1X2Y.shape[0])\n",
    "    grad_cost[2] = np.sum(error*SX0X1X2Y[:,2:3])/(SX0X1X2Y.shape[0])\n",
    "    return grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SX0X1X2Y = sampleData()\n",
    "# gradientFunction(SX0X1X2Y,[3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Stochastic gradient descent unitil it converges\n",
    "\n",
    "def stochasticGradientDescent(SX0X1X2Y):\n",
    "    \n",
    "    \n",
    "    \n",
    "    theta = np.zeros((3,1))  # initializing theta vector\n",
    "\n",
    "    learningRate = 1e-3\n",
    "\n",
    "    \n",
    "    batchSize = 1000000  # size of batch on which we would run SGD\n",
    "    Batch_SX0X1X2Y = np.reshape(SX0X1X2Y,(-1,batchSize,4))\n",
    "    \n",
    "    print(\"batch size is {}\".format(batchSize))\n",
    "\n",
    "\n",
    "    stopping_criteria  = 0\n",
    "    avgIterations = 0\n",
    "    if batchSize < 1001:\n",
    "        stopping_criteria = 1e-4\n",
    "        avgIterations = 2000\n",
    "    else:\n",
    "        stopping_criteria = 1e-3\n",
    "        avgIterations = 1000\n",
    "\n",
    "    \n",
    "    print(\"stopping_criteria is  {}\".format(stopping_criteria))\n",
    "    print(\"avgIterations is  {}\".format(avgIterations))\n",
    "    threshold = 1000000    \n",
    "    iterations = 0\n",
    "    count = 0\n",
    "    costNew = 0\n",
    "    cost = 0\n",
    "    thetaList = [] # stroing theta in list to for plotting purpose\n",
    "    while(True):\n",
    "    \n",
    "        for i in Batch_SX0X1X2Y:\n",
    "            iterations += 1\n",
    "            thetaList.append(theta.copy())\n",
    "            cost += costFuction(i,theta)\n",
    "            count += 1\n",
    "            \n",
    "            if count == avgIterations:\n",
    "                \n",
    "                costOld = costNew\n",
    "                costNew = cost/avgIterations\n",
    "                cost = 0\n",
    "                count = 0\n",
    "                \n",
    "                if abs(costOld - costNew) <= stopping_criteria:\n",
    "                    return theta,iterations,thetaList\n",
    "                \n",
    "                \n",
    "            if iterations > threshold:\n",
    "                return theta,iterations,thetaList\n",
    "            \n",
    "            theta -= learningRate * gradientFunction(i,theta)\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    SX0X1X2Y = sampleData()\n",
    "    startTime = time()\n",
    "    (theta,iterations,thetaList) = stochasticGradientDescent(SX0X1X2Y)\n",
    "    endTime = time() -startTime\n",
    "    thetaList=np.array(thetaList).reshape(-1,3)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"theta is: {}\".format(theta.tolist()))\n",
    "    print(\"total iterations took to converge is: {}\".format(iterations))\n",
    "\n",
    "    print(\"total time took to converge is: {}\".format(endTime))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set(xlim=(0, 4), ylim=(0, 2), zlim = (0,3))\n",
    "#     print(thetaList)\n",
    "    thetaList0 = thetaList[:,0].tolist()\n",
    "    thetaList1 = thetaList[:,1].tolist()\n",
    "    thetaList2 = thetaList[:,2].tolist()\n",
    "\n",
    "\n",
    "    lines, = ax.plot([], [],[],color = 'green',markersize = 2)\n",
    "    def animationLine(i):\n",
    "            lines.set_data(thetaList0[:i],thetaList1[:i])\n",
    "            lines.set_3d_properties(thetaList2[:i])\n",
    "            lines.set_marker(\"x\")\n",
    "            return lines,\n",
    "    anim = animation.FuncAnimation(fig, animationLine,frames=len(thetaList0),interval = 1,repeat=False)\n",
    "    plt.xlabel(\"theta 0\",color ='r')\n",
    "    plt.ylabel(\"theta 1\",color ='r')\n",
    "    ax.set_zlabel(\"theta 2\",color ='r')\n",
    "    plt.title(\"Movement of parameters\")\n",
    "    plt.show()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 100\n",
      "stopping_criteria is  0.0001\n",
      "avgIterations is  2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9847285189927921"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=plot()\n",
    "SX0X1X2Y = sampleData()\n",
    "(theta,iterations,thetaList) = stochasticGradientDescent(SX0X1X2Y)\n",
    "X0X1X2Y=inputData()\n",
    "costFuction(X0X1X2Y,theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
