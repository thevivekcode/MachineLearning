{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputData():\n",
    "    df = pd.read_csv(\"./data/q2/q2test.csv\")\n",
    "    X_0 = np.ones((len(df),1))\n",
    "    X1X2Y = df.to_numpy()\n",
    "    X0X1X2Y = np.append(X_0,X1X2Y, axis=1)\n",
    "    np.random.shuffle(X0X1X2Y)  #shuffling data to make it random for better distribution\n",
    "    return X0X1X2Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData():\n",
    "    Stheta = np.reshape(np.array([3,1,2]),(3,1))\n",
    "    SX_0 = np.ones((1000000))\n",
    "    SX_1 = np.random.normal(size = 1000000, loc = 3, scale = 2) # loc = mean scale = standard deviation\n",
    "    SX_2 = np.random.normal(size = 1000000, loc = -1, scale = 2)\n",
    "    SX = np.reshape(np.array([SX_0,SX_1,SX_2]).T,(-1,3) ) # creating design matrix of sample data\n",
    "    hypoT = np.dot(SX,Stheta)\n",
    "#     print(\"hypoThesis shape\")\n",
    "#     print(hypoT.shape)\n",
    "    SY = np.reshape(np.random.normal(size = 1000000, loc = hypoT.T , scale = math.sqrt(2)),(-1,1))\n",
    "    SX0X1X2Y = np.append(SX,SY,axis=1)\n",
    "#     print(SX0X1X2Y)\n",
    "    np.random.shuffle(SX0X1X2Y)\n",
    "#     print(SX0X1X2Y)\n",
    "#     plot to visualize the generated sample\n",
    "#     plt.scatter(SX0X1X2Y[:,1:2],SX0X1X2Y[:,3:4])\n",
    "#     plt.scatter(SX0X1X2Y[:,2:3],SX0X1X2Y[:,3:4])\n",
    "#     plt.show()\n",
    "\n",
    "    return SX0X1X2Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFuction(SX0X1X2Y,theta):\n",
    "    hypothesis = np.dot(SX0X1X2Y[:,0:3],np.array(theta))\n",
    "    hypothesis = np.reshape(hypothesis,(-1,1))\n",
    "    error = SX0X1X2Y[:,3:4]-hypothesis\n",
    "    errorSquared =error**2\n",
    "    examples = SX0X1X2Y.shape[0] #X.shape[0] represents number of training example\n",
    "    return np.sum(errorSquared)/(2*examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = sampleData()\n",
    "# print(a)\n",
    "# costFuction(a,[3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFunction(SX0X1X2Y,theta):\n",
    "    error = np.reshape(np.reshape(np.dot(SX0X1X2Y[:,0:3],np.array(theta)),(-1,1)) - SX0X1X2Y[:,3:4],(-1,1))  # h(X) -y\n",
    "    grad_cost = np.zeros((3,1))\n",
    "    grad_cost[0] = np.sum(error*SX0X1X2Y[:,0:1])/(SX0X1X2Y.shape[0])\n",
    "    grad_cost[1] = np.sum(error*SX0X1X2Y[:,1:2])/(SX0X1X2Y.shape[0])\n",
    "    grad_cost[2] = np.sum(error*SX0X1X2Y[:,2:3])/(SX0X1X2Y.shape[0])\n",
    "    return grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SX0X1X2Y = sampleData()\n",
    "# gradientFunction(SX0X1X2Y,[3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Stochastic gradient descent unitil it converges\n",
    "\n",
    "def stochasticGradientDescent(SX0X1X2Y):\n",
    "    \n",
    "    \n",
    "    \n",
    "    theta = np.zeros((3,1))  # initializing theta vector\n",
    "\n",
    "    learningRate = 1e-3\n",
    "    \n",
    "    \n",
    "    costPlot = [] # list for plot of costfunction\n",
    "    thetaList0 = []\n",
    "    thetaList1 = []\n",
    "    thetaList2 = []\n",
    "    \n",
    "    batchSize = 1000000  # size of batch on which we would run SGD\n",
    "    Batch_SX0X1X2Y = np.reshape(SX0X1X2Y,(-1,batchSize,4))\n",
    "    \n",
    "    print(\"batch size is {}\".format(batchSize))\n",
    "\n",
    "\n",
    "    stopping_criteria  = 0\n",
    "    if len(Batch_SX0X1X2Y) <=10000:\n",
    "        stopping_criteria = 1e-3\n",
    "    else:\n",
    "        stopping_criteria = 1e-3\n",
    "        \n",
    "#     repeatIteration = 0\n",
    "#     if len(Batch_SX0X1X2Y) <=10000:\n",
    "#         repeatIteration = 100\n",
    "#     else:\n",
    "#         repeatIteration = 1000\n",
    "    \n",
    "    print(\"stopping_criteria is  {}\".format(stopping_criteria))\n",
    "    \n",
    "    count = 0\n",
    "    costNew = 0\n",
    "    cost = 0\n",
    "    flag = 0\n",
    "    while(True):\n",
    "    \n",
    "        for i in Batch_SX0X1X2Y:\n",
    "            theta -= learningRate * gradientFunction(i,theta)\n",
    "            if flag ==1:\n",
    "                cost += costFuction(i,theta)\n",
    "                count += 1\n",
    "                if count == 1000:\n",
    "                    costOld = costNew\n",
    "                    costNew = cost/1000\n",
    "                    print(abs(costOld - costNew))\n",
    "    #                 print(theta)\n",
    "                    if abs(costOld - costNew) <= stopping_criteria:\n",
    "                        return theta\n",
    "                    cost = 0\n",
    "                    count = 0\n",
    "        flag = 1\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 1000000\n",
      "stopping_criteria is  0.001\n",
      "3.2890756992151737\n",
      "1.7929464269078597\n",
      "0.20774432016067435\n",
      "0.12051788105350103\n",
      "0.07005180196749872\n",
      "0.04071811114439772\n",
      "0.023667693473634843\n",
      "0.013757016193006955\n",
      "0.007996364104747355\n",
      "0.004647943856323611\n",
      "0.002701650626280472\n",
      "0.0015703537590192607\n",
      "0.0009127793595766764\n",
      "theta is [[2.91955628]\n",
      " [1.01823317]\n",
      " [1.99445098]]\n",
      "470.91006422042847\n"
     ]
    }
   ],
   "source": [
    "SX0X1X2Y = sampleData()\n",
    "startTime = time()\n",
    "theta = stochasticGradientDescent(SX0X1X2Y)\n",
    "print(\"theta is {}\".format(theta))\n",
    "endTime = time() -startTime\n",
    "print(endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SX0X1X2Y = sampleData()\n",
    "# theta = [3,1,2]\n",
    "# costFuction(SX0X1X2Y,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 5 1]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4,5])\n",
    "np.random.shuffle(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
