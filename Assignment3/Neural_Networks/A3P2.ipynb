{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"./Alphabets/train.csv\",header=None).to_numpy()\n",
    "test_data = pd.read_csv(\"./Alphabets/test.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:,:-1]/255\n",
    "train_y = train_data[:,-1]\n",
    "\n",
    "test_x = test_data[:,:-1]/255\n",
    "test_y = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self,batchSize,input_features,architecture,target_class,eta,epoch):\n",
    "        self.batchSize = batchSize\n",
    "        self.input_features = input_features\n",
    "        self.architecture = architecture\n",
    "        self.target_class = target_class\n",
    "\n",
    "        self.parameter = []\n",
    "        self.layer_input = [0]*(len(architecture)+1)\n",
    "        self.layer_output = [0]*(len(architecture)+1)\n",
    "        self.layer_delta = [0]*(len(architecture)+1)\n",
    "        self.total_layers = len(architecture)+1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "\n",
    "\n",
    "    def activation(self,x,mode):\n",
    "        if(mode==\"sigmoid\"):\n",
    "            return 1/(1+np.exp(-x))\n",
    "#         if(mode==\"relu\")\n",
    "#             return\n",
    "\n",
    "    def oneHotEncoding(self,y): #y is **list** of ouput label 0<=y<26\n",
    "        \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "        a = np.array(y)\n",
    "        b = np.zeros((a.size, 26))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return b\n",
    "\n",
    "\n",
    "    def initalize_parameters(self):\n",
    "\n",
    "        total_layers_architecture = self.architecture  + [self.target_class]\n",
    "        layerInputSize = self.input_features\n",
    "        np.random.seed(0)\n",
    "        \"\"\"first hidden layer starts with 0 \"\"\"\n",
    "        for layer, total_neurons in enumerate(total_layers_architecture):\n",
    "\n",
    "            layerOutputSize = total_neurons\n",
    "\n",
    "            layer_weight = np.random.uniform(low=-0.1, high=0.1, size=(layerOutputSize, layerInputSize))\n",
    "            layer_bias  = np.random.uniform(low=-0.1, high=0.1, size=(layerOutputSize,1))\n",
    "\n",
    "            self.parameter.append([layer_weight,layer_bias])\n",
    "\n",
    "            layerInputSize = layerOutputSize\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def full_feedForward(self,X):\n",
    "        current_input = X.copy()\n",
    "\n",
    "        for layer in range(self.total_layers):\n",
    "\n",
    "            current_parameter = self.parameter[layer]\n",
    "\n",
    "            weight = current_parameter[0]\n",
    "            bias = current_parameter[1]\n",
    "            netJ = np.dot(current_input,weight.T) + bias.T\n",
    "            G_netJ  = self.activation(netJ,\"sigmoid\")   # activetion applied => g(netj)\n",
    "\n",
    "            self.layer_output[layer] = G_netJ\n",
    "            self.layer_input[layer] = current_input\n",
    "\n",
    "            current_input = G_netJ.copy()\n",
    "        \"\"\"         last single layer output is the output of entire neural network\n",
    "                     to be used for calculating new loss function value                   \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_backpropagation(self,Y):\n",
    "\n",
    "        \"\"\"  start with last (ouyput)layer whose deltaJ is calculated\n",
    "             differently then rest of hidden layer                    \"\"\"\n",
    "        lastlayer = self.total_layers-1\n",
    "        op = self.layer_output[lastlayer]\n",
    "        deltaJ_lastlayer = (Y-op)*op*(1-op)/(Y.shape[0])\n",
    "        self.layer_delta[lastlayer] = deltaJ_lastlayer\n",
    "\n",
    "        deltaJ_prev = deltaJ_lastlayer.copy()\n",
    "\n",
    "        #reverse iteration\n",
    "        for layer in range(self.total_layers-1,0,-1):\n",
    "            theta_downNBR  = self.parameter[layer][0] # weight without bias\n",
    "            oj = self.layer_output[layer-1]\n",
    "            deltaJ_curr = np.dot(deltaJ_prev, theta_downNBR)*oj*(1-oj)\n",
    "            self.layer_delta[layer-1] = deltaJ_curr\n",
    "            deltaJ_prev = deltaJ_curr.copy()\n",
    "\n",
    "    def costFunction(self,y):\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return (np.sum((y-final_op)**2))/(2*y.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def updateParameters(self):\n",
    "\n",
    "        for i in range(len(self.architecture)+1):\n",
    "            \n",
    "            gradient_W = np.dot(self.layer_delta[i].T, self.layer_input[i])\n",
    "            gradient_B = np.sum(self.layer_delta[i],axis = 0).T.reshape((-1,1))\n",
    "#             gradient = np.dot(self.layer_delta[i].T, np.append(np.ones((self.layer_input[i].shape[0],1)),self.layer_input[i],axis = 1))\n",
    "            self.parameter[i][0] = self.parameter[i][0] + (self.eta)*gradient_W\n",
    "            self.parameter[i][1] = self.parameter[i][1] + (self.eta)*gradient_B\n",
    "\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        self.initalize_parameters()\n",
    "        \n",
    "        Y = self.oneHotEncoding(y)\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            totalBatches = math.ceil(x.shape[0]/self.batchSize)\n",
    "            for j in range(0,x.shape[0],self.batchSize):\n",
    "                \n",
    "                x_batch = x[j:j+self.batchSize]\n",
    "                y_batch = Y[j:j+self.batchSize]\n",
    "\n",
    "                self.full_feedForward(x_batch)\n",
    "               \n",
    "                self.full_backpropagation(y_batch)\n",
    "\n",
    "                self.updateParameters()\n",
    "\n",
    "            if i%100==0:\n",
    "                print(\"loss value is \",self.costFunction(y_batch))\n",
    "\n",
    "\n",
    "\n",
    "    def score(self,x,y):\n",
    "        self.full_feedForward(x)\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return np.count_nonzero((np.argmax(final_op,axis =1) == y) == True)/y.shape[0],np.argmax(final_op,axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchSize,input_features,architecture,target_class,eta,epoch\n",
    "model = neuralNetwork(100,784,[100],26,2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value is  0.4781705120837138\n",
      "46.66001892089844\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "model.fit(train_x,train_y)\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9685384615384616, array([24,  5, 24, ...,  7, 15, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.908923076923077, array([25, 13,  6, ...,  3,  4, 12]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
