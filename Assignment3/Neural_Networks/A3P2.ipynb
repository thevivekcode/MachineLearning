{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"./Alphabets/train.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:,:-1].astype('float128')\n",
    "train_y = train_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e75aa10f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"data consists of 26 english alphabets\"\"\"\n",
    "plt.imshow(train_x[28,:].astype('int').reshape((28,28)),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(y): #y is list of ouput label 0<=y<26\n",
    "    \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "    a = np.array(y)\n",
    "    b = np.zeros((a.size, 26))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Assuming fully connected network\"\"\"\n",
    "    \n",
    "    def __inti__(self, mini_batch_size, total_input_features, architecture, total_target_class):\n",
    "        self.mini_batch_size  = mini_batch_size\n",
    "        self.total_input_features = total_input_features\n",
    "        self.architecture = architecture # this is list of nodes in hidden layer\n",
    "        self.total_target_class = total_target_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_parameters(total_input_features , architecture, total_target_class):\n",
    "    parameter = dict()\n",
    "    total_layers_architecture = architecture  + [total_target_class]\n",
    "    \n",
    "    for layer, total_neurons in enumerate(total_layers_architecture):\n",
    "        layerNumber = layer+1\n",
    "        layerInputSize = total_input_features + 1\n",
    "        layerOutputSize = total_neurons\n",
    "        \"\"\"random floats sampled from a univariate *normal* (Gaussian)\n",
    "            distribution of mean 0 and variance 1\"\"\"\n",
    "        layer_parameter = np.random.randn(layerOutputSize, layerInputSize)*0.1\n",
    "#         print(layerNumber)\n",
    "        parameter[layerNumber] = layer_parameter\n",
    "        total_input_features = layerOutputSize\n",
    "    \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_feedforward(current_input,parameters):\n",
    "    total_examples = current_input.shape[0]\n",
    "    X = np.append(np.ones((total_examples,1)),current_input,axis=1)\n",
    "    netJ = np.dot(X,parameters.T)\n",
    "    opJ  = sigmoid(netJ)\n",
    "    return opJ,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_feedForward(X,architecture,total_target_class):\n",
    "    layer_input = dict()\n",
    "    layer_ouput = dict()\n",
    "    current_input = X \n",
    "    total_layers_architecture = architecture  + [total_target_class]\n",
    "    \n",
    "    for layer,totalNeuron in enumerate(total_layers_architecture):\n",
    "        layerNumber = layer+1 # hidden layer\n",
    "        current_parameter = parameter[layerNumber]\n",
    "        singleLayer_output,singleLayer_input = layer_feedforward(current_input, current_parameter)\n",
    "        print(singleLayer_output.shape)\n",
    "        layer_ouput[layerNumber] = singleLayer_output\n",
    "        layer_input[layerNumber] = singleLayer_input\n",
    "        current_input = singleLayer_output\n",
    "    \n",
    "    return layer_ouput, layer_input, singleLayer_output\n",
    "\n",
    "    \"\"\"         last single layer output is the output of entire neural network\n",
    "                 to be used for calculating new loss function value                   \"\"\"\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "(50, 100)\n",
      "(50, 26)\n"
     ]
    }
   ],
   "source": [
    "parameter=initalize_parameters(784,[50,100],26)\n",
    "layer_ouput, layer_input, final_op = full_feedForward(train_x[:50,:],[52,52],26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(y,layer_ouput):\n",
    "    Y = oneHotEncoding(y)\n",
    "    return (np.sum((Y-layer_ouput)**2))/(Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.981437458299956521"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costFunction(train_y[:50],final_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backpropagation(train_y,final_op,architecture):\n",
    "    DeltaJ_output = dict()\n",
    "    \"\"\"  start with last (ouyput)layer whose deltaJ is calculated \n",
    "          differently then rest of hidden layer                    \"\"\"\n",
    "    Y  = oneHotEncoding(train_y)\n",
    "    deltaJ_prev = (Y-final_op)*final_op*(1-final_op)\n",
    "    DeltaJ_output[len(architecture)+1] = deltaJ_prev\n",
    "    \n",
    "    for layer,value in reversed(list(enumerate(architecture))):\n",
    "        theta_downNBR  = parameter[layer+2][:,1:] # removing bias term of theta 1.e col 0 of parameter\n",
    "        deltaJ_curr = np.dot(deltaJ_prev, theta_downNBR)*layer_ouput[layer+1]\n",
    "        DeltaJ_output[layer+1] =deltaJ_curr\n",
    "        deltaJ_prev = deltaJ_curr\n",
    "    \n",
    "    return DeltaJ_output\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaJ_output = full_backpropagation(train_y[:50],final_op,[50,100])\n",
    "DeltaJ_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
