{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"./Alphabets/train.csv\",header=None).to_numpy()\n",
    "test_data = pd.read_csv(\"./Alphabets/test.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:,:-1]/255\n",
    "train_y = train_data[:,-1]\n",
    "test_x = test_data[:,:-1]/255\n",
    "test_y = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class\n",
    "## PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training algorithms for deep learning models are usually iterative in nature and thus \n",
    "require the user to specify some initial point from which to begin the iterations. \n",
    "Moreover, training deep models is a sufficiently difficult task that most algorithms are\n",
    "strongly affected by the choice of initialization. \n",
    "above quote is not mine, I read it online while trying to understand the initialization part\"\"\"\n",
    "\n",
    "class neuralNetwork:\n",
    "    def __init__(self,batchSize,input_features,architecture,target_class,eta,max_iter,activationMode,learningRate):\n",
    "        self.batchSize = batchSize\n",
    "        self.input_features = input_features\n",
    "        self.architecture = architecture\n",
    "        self.target_class = target_class\n",
    "        self.learningRate = learningRate\n",
    "        self.activationMode  = activationMode\n",
    "\n",
    "        self.parameter = []\n",
    "        self.layer_input = [0]*(len(architecture)+1)\n",
    "        self.layer_output = [0]*(len(architecture)+1)\n",
    "        self.layer_delta = [0]*(len(architecture)+1)\n",
    "        self.total_layers = len(architecture)+1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        \n",
    "\n",
    "    def activation(self,x,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif mode==\"relu\":\n",
    "            return np.where(x<0,0,x)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def differentiation(self,op,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return op*(1-op)\n",
    "        elif mode==\"relu\":\n",
    "            return 1 * (op > 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def oneHotEncoding(self,y): #y is **list** of ouput label 0<=y<26\n",
    "        \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "        a = np.array(y)\n",
    "        b = np.zeros((a.size, self.target_class))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return b\n",
    "\n",
    "    \n",
    "\n",
    "    def initalize_parameters(self):\n",
    "\n",
    "        total_layers_architecture = self.architecture  + [self.target_class]\n",
    "        layerInputSize = self.input_features\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        \"\"\"Random intialization is used to preserve the stochastic nature of neural networks\"\"\"\n",
    "        \n",
    "        for layer, total_neurons in enumerate(total_layers_architecture):\n",
    "            np.random.seed(layer)\n",
    "            if layer ==0:\n",
    "                neurons_in_prev = self.input_features\n",
    "            else:\n",
    "                neurons_in_prev = total_layers_architecture[layer-1]\n",
    "                 \n",
    "            \n",
    "            layerOutputSize = total_neurons\n",
    "            layer_weight = np.random.randn(layerOutputSize, layerInputSize)/math.sqrt(neurons_in_prev)\n",
    "            \"\"\"It is important to note that the bias weight in each neuron\n",
    "            is set to zero by default, not a small random value.\"\"\"\n",
    "            layer_bias  = np.zeros((layerOutputSize,1))\n",
    "\n",
    "#             layer_weight = np.random.uniform(low=-0.025, high=0.025, size=(layerOutputSize, layerInputSize))\n",
    "#             layer_bias  = np.zeros((layerOutputSize,1))\n",
    "            \n",
    "            self.parameter.append([layer_weight,layer_bias])\n",
    "\n",
    "            layerInputSize = layerOutputSize\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def full_feedForward(self,X):\n",
    "        current_input = X.copy()\n",
    "\n",
    "        for layer in range(self.total_layers):\n",
    "\n",
    "            current_parameter = self.parameter[layer]\n",
    "\n",
    "            weight = current_parameter[0]\n",
    "            bias = current_parameter[1]\n",
    "            netJ = np.dot(current_input,weight.T) + bias.T\n",
    "            \n",
    "            if self.activationMode == \"relu\":\n",
    "                if layer == self.total_layers-1:\n",
    "                    G_netJ  = self.activation(netJ,\"sigmoid\")   # only output layer \n",
    "                else :\n",
    "                    G_netJ  = self.activation(netJ,\"relu\")   # all hidden layer\n",
    "            \n",
    "            elif self.activationMode == \"sigmoid\":\n",
    "                G_netJ  = self.activation(netJ,\"sigmoid\") # all layers \n",
    "                    \n",
    "\n",
    "            self.layer_output[layer] = G_netJ\n",
    "            self.layer_input[layer] = current_input\n",
    "\n",
    "            current_input = G_netJ.copy()\n",
    "        \"\"\"         last single layer output is the output of entire neural network\n",
    "                     to be used for calculating new loss function value                   \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_backpropagation(self,Y):\n",
    "\n",
    "        \"\"\"  start with last (ouyput)layer whose deltaJ is calculated\n",
    "             differently then rest of hidden layer                    \"\"\"\n",
    "        lastlayer = self.total_layers-1\n",
    "        op = self.layer_output[lastlayer]\n",
    "        \n",
    "        diff_op = self.differentiation(op,\"sigmoid\")  # in every case output layer is sigmoid\n",
    "        \n",
    "        deltaJ_lastlayer = (Y-op)*diff_op/(Y.shape[0])\n",
    "        self.layer_delta[lastlayer] = deltaJ_lastlayer\n",
    "\n",
    "        deltaJ_prev = deltaJ_lastlayer.copy()\n",
    "\n",
    "        #reverse iteration\n",
    "        for layer in range(self.total_layers-1,0,-1):\n",
    "            theta_downNBR  = self.parameter[layer][0] # weight without bias\n",
    "\n",
    "            oj = self.layer_output[layer-1]\n",
    "            \n",
    "            if self.activationMode ==\"relu\":\n",
    "                diff_oj = self.differentiation(oj,\"relu\")\n",
    "            elif self.activationMode==\"sigmoid\":\n",
    "                diff_oj = self.differentiation(oj,\"sigmoid\")\n",
    "                \n",
    "            deltaJ_curr = np.dot(deltaJ_prev, theta_downNBR)*diff_oj\n",
    "\n",
    "            self.layer_delta[layer-1] = deltaJ_curr\n",
    "            deltaJ_prev = deltaJ_curr.copy()\n",
    "            \n",
    "            \n",
    "\n",
    "    def costFunction(self,y):\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return (np.sum((y-final_op)**2))/(2*y.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def updateParameters(self,epochCount):\n",
    "        ETA = self.eta   #defalut for fault tolerance\n",
    "        \n",
    "        if self.learningRate == \"normal\":\n",
    "            ETA = self.eta\n",
    "        elif self.learningRate ==  \"adaptive\":\n",
    "            ETA = self.eta/math.sqrt(epochCount) # as per question requirement \n",
    "            \n",
    "        for i in range(len(self.architecture)+1):\n",
    "            \n",
    "            gradient_W = np.dot(self.layer_delta[i].T, self.layer_input[i])\n",
    "            gradient_B = np.sum(self.layer_delta[i],axis = 0).T.reshape((-1,1))\n",
    "            self.parameter[i][0] = self.parameter[i][0] + (ETA)*gradient_W\n",
    "            self.parameter[i][1] = self.parameter[i][1] + (ETA)*gradient_B\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        self.initalize_parameters()\n",
    "        \n",
    "        indexes = np.arange(x.shape[0])\n",
    "        Y = self.oneHotEncoding(y)\n",
    "        \n",
    "        cost, newcost =0, 0\n",
    "        improvement, old_improvement = math.inf , 0\n",
    "        n_iter_no_change = 3\n",
    "        i = 0\n",
    "        epochNumber = 0\n",
    "        totalBatches = math.ceil(x.shape[0]/self.batchSize)\n",
    "        while(True):\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "            \"\"\"shuffle the data after every_epoch to maintain stochastic nature(random) of the newtork\"\"\"\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "            epochNumber = i\n",
    "            \n",
    "            for j in range(0,x.shape[0],self.batchSize):\n",
    "                \n",
    "                batch = indexes[j:j + self.batchSize]\n",
    "                \n",
    "                x_batch = x[batch]\n",
    "                y_batch = Y[batch]\n",
    "\n",
    "                self.full_feedForward(x_batch)\n",
    "               \n",
    "                self.full_backpropagation(y_batch)\n",
    "\n",
    "                self.updateParameters(epochNumber)\n",
    "                \n",
    "                cost += self.costFunction(y_batch)\n",
    "\n",
    "            oldcost = newcost\n",
    "            newcost = cost/totalBatches\n",
    "            cost = 0\n",
    "            \n",
    "            old_improvement = improvement\n",
    "            improvement  = abs(oldcost - newcost)\n",
    "            \n",
    "            if improvement < 1e-5:\n",
    "                n_iter_no_change-=1\n",
    "                if n_iter_no_change ==0:\n",
    "                    print(\"convergence reached with total epoch :\",i)\n",
    "                    return i\n",
    "                    break\n",
    "            else:\n",
    "                n_iter_no_change = 3\n",
    "                \n",
    "\n",
    "            if i == self.max_iter:\n",
    "                print(\"max_iter reached\")\n",
    "                return i\n",
    "                break   \n",
    "                \n",
    "            if i%100 ==0:\n",
    "                print(improvement)\n",
    "                print('Current Epoch is : ',i)\n",
    "                \n",
    "                \n",
    "\n",
    "    def score(self,x,y):\n",
    "        self.full_feedForward(x)\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return np.count_nonzero((np.argmax(final_op,axis =1) == y) == True)/y.shape[0] #,np.argmax(final_op,axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_b():\n",
    "    arch  = [1,5,10,50,100]\n",
    "    train_acc_b = []\n",
    "    test_acc_b = []\n",
    "    time_list_b = []\n",
    "    epoch_b = []\n",
    "    for i in tqdm(arch):\n",
    "        startTime = time.time()\n",
    "        model_b = neuralNetwork(100,784,[i],26,0.1,3000,\"sigmoid\",\"normal\")\n",
    "        epoch_b.append(model_b.fit(train_x,train_y))\n",
    "        time_list_b.append(time.time()-startTime)\n",
    "        train_acc_b.append(model_b.score(train_x,train_y))\n",
    "        test_acc_b.append(model_b.score(test_x,test_y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    labels = ['1', '5', '10', '50', '100']\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    acc_b1 = ax1.plot(x, np.array(test_acc_b)*100,color = \"blue\")\n",
    "    acc_b2 = ax1.plot(x, np.array(train_acc_b)*100,color = \"#ff7f0e\")\n",
    "    acc_b3 = ax1.bar(x - width/2, np.array(train_acc_b)*100, width, label='train accuracy',color = \"#ff7f0e\")\n",
    "    acc_b4 = ax1.bar(x + width/2, np.array(test_acc_b)*100, width, label='test accuracy',color = \"blue\")\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax1.set_title('Sigmoid and normal learningMode')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.legend()\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    t = ax2.bar(x, np.array(time_list_b)/60, width, label='Time: minutes',color = \"blue\")\n",
    "    ax2.set_ylabel('Time taken to converge')\n",
    "    ax2.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax2.set_title('Sigmoid and normal learningMode')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(labels)\n",
    "    ax2.legend()\n",
    "\n",
    "    fig2_2 = plt.figure()\n",
    "    ax2_2 = fig2_2.add_subplot(111)\n",
    "    epo = ax2_2.bar(x, epoch_b , width, label='epochs',color=\"blue\")\n",
    "    ax2_2.set_ylabel('Epoch/iteration to reach convergence')\n",
    "    ax2_2.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax2_2.set_title('Sigmoid and normal learningMode')\n",
    "    ax2_2.set_xticks(x)\n",
    "    ax2_2.set_xticklabels(labels)\n",
    "    ax2_2.legend()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    return train_acc_b, test_acc_b, time_list_b, epoch_b, acc_b1, acc_b2, acc_b3,acc_b4,t,epo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_c():\n",
    "    arch  = [1,5,10,50,100]\n",
    "    train_acc_c = []\n",
    "    test_acc_c = []\n",
    "    time_list_c = []\n",
    "    epoch_c = []\n",
    "    for i in tqdm(arch):\n",
    "        startTime = time.time()\n",
    "        model_c = neuralNetwork(100,784,[i],26,0.5,3000,\"sigmoid\",\"adaptive\")\n",
    "        epoch_c.append(model_c.fit(train_x,train_y))\n",
    "        time_list_c.append(time.time()-startTime)\n",
    "        train_acc_c.append(model_c.score(train_x,train_y))\n",
    "        test_acc_c.append(model_c.score(test_x,test_y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    labels = ['1', '5', '10', '50', '100']\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig3 = plt.figure()\n",
    "    ax3 = fig3.add_subplot(111)\n",
    "    acc_c1 = ax3.plot(x, np.array(train_acc_c)*100,color = \"#ff7f0e\")\n",
    "    acc_c2 = ax3.plot(x, np.array(test_acc_c)*100,color = \"blue\")\n",
    "    acc_c3 = ax3.bar(x - width/2, np.array(train_acc_c)*100, width, label='train accuracy')\n",
    "    acc_c4 = ax3.bar(x + width/2, np.array(test_acc_c)*100, width, label='test accuracy')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax3.set_title('Sigmoid and adaptive learningMode')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(labels)\n",
    "    ax3.legend()\n",
    "\n",
    "\n",
    "    fig4 = plt.figure()\n",
    "    ax4 = fig4.add_subplot(111)\n",
    "    t = ax4.bar(x, np.array(time_list_c)/60, width, label='Time: minutes')\n",
    "    ax4.set_ylabel('Time taken to converge')\n",
    "    ax4.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax4.set_title('Sigmoid and adaptive learningMode')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(labels)\n",
    "    ax4.legend()\n",
    "\n",
    "    fig4_2 = plt.figure()\n",
    "    ax4_2 = fig4_2.add_subplot(111)\n",
    "    epo = ax4_2.bar(x, epoch_c , width, label='epochs')\n",
    "    ax4_2.set_ylabel('Epoch/iteration to reach convergence')\n",
    "    ax4_2.set_xlabel(\"Neurons in single hidden layer\")\n",
    "    ax4_2.set_title('Sigmoid and adaptive learningMode')\n",
    "    ax4_2.set_xticks(x)\n",
    "    ax4_2.set_xticklabels(labels)\n",
    "    ax4_2.legend()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    return train_acc_c, test_acc_c, time_list_c, epoch_c, acc_c1, acc_c2, acc_c3, acc_c4, t, epo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_d():\n",
    "    print(\"-----------------Part D started------------------------------\")\n",
    "    model_d_sigmoid = neuralNetwork(100,784,[100,100],26,0.5,3000,\"sigmoid\",\"adaptive\")\n",
    "    \n",
    "    s = time.time()\n",
    "    epoch_d_sigmoid = model_d_sigmoid.fit(train_x, train_y)\n",
    "    print(\"time taken for sigmoid is :\", time.ime()-s)\n",
    "    \n",
    "    train_acc_d_sigmoid = model_d_sigmoid.score(train_x, train_y)\n",
    "    test_acc_d_sigmoid = model_d_sigmoid.score(test_x,test_y)\n",
    "    print(\"train accuray is : sigmoid \",train_acc_d_sigmoid)\n",
    "    print(\"test accuray is : sigmid  \",test_acc_d_sigmoid)\n",
    "\n",
    "    model_d_relu = neuralNetwork(100,784,[100,100],26,0.5,3000,\"relu\",\"adaptive\")\n",
    "\n",
    "    s = time.time()\n",
    "    epoch_d_relu = model_d_relu.fit(train_x, train_y)\n",
    "    print(\"time taken for relu is :\", time.ime()-s)\n",
    "    \n",
    "    train_acc_d_relu = model_d_relu.score(train_x, train_y)\n",
    "    test_acc_d_relu = model_d_relu.score(test_x,test_y)\n",
    "    print(\"train accuray is : Relu \",train_acc_d_relu)\n",
    "    print(\"test accuray is : Relu \",test_acc_d_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Part E started------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------Part E started------------------------------\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def oneHotEncoding(y,targetClass): #y is **list** of ouput label 0<=y<26\n",
    "    \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "    a = np.array(y)\n",
    "    b = np.zeros((a.size,targetClass ))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_e():\n",
    "    Y = oneHotEncoding(train_y,26)\n",
    "\n",
    "    model_e = MLPClassifier(activation=\"relu\",hidden_layer_sizes=(100,100,),solver='sgd',alpha= 0.0,\n",
    "                            batch_size=100, learning_rate='invscaling',learning_rate_init=0.5,max_iter=2000,\n",
    "                            random_state = 0, momentum = 0.0,verbose=False)\n",
    "    model_e.fit(train_x,Y)\n",
    "\n",
    "    prob_train = model_e.predict_proba(train_x)\n",
    "    prob_test = model_e.predict_proba(test_x)\n",
    "\n",
    "    print(accuracy_score(train_y,prob_train.argmax(axis = 1))*100)\n",
    "    print(accuracy_score(test_y,prob_test.argmax(axis = 1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.38461538461539\n",
      "84.39999999999999\n"
     ]
    }
   ],
   "source": [
    "part_e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when i thought we need to train 26 models for each class as binary classification \n",
    "\n",
    "# models = []\n",
    "# for i in tqdm(range(26)):\n",
    "#     model = MLPClassifier(hidden_layer_sizes=(100,100,), activation='relu', solver='lbfgs', \n",
    "#                       alpha=0.0001, batch_size=100, learning_rate='adaptive', \n",
    "#                       learning_rate_init=0.5, power_t=0.5, max_iter=200, \n",
    "#                       shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "#                       warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "#                       validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "#                       n_iter_no_change=10, max_fun=15000)\n",
    "#     model.fit(train_x,Y[:,i])\n",
    "#     models.append(model)\n",
    "    \n",
    "# predictions_prob_train = []\n",
    "# predictions_prob_test = []\n",
    "# for i in range(26):\n",
    "#     model  = models[i]\n",
    "#     predictions_prob_train.append(model.predict_proba(train_x))\n",
    "#     predictions_prob_test.append(model.predict_proba(test_x))\n",
    "\n",
    "    \n",
    "# def prediction(x,predictions_prob)\n",
    "#     prediction= []\n",
    "#     for j in range(x.shape[0]):\n",
    "#         prob = 0\n",
    "#         Class = -1\n",
    "#         for i in range(26):\n",
    "\n",
    "#             if prob < predictions_prob[i][j][1]:\n",
    "#                 Class = i\n",
    "#                 prob = predictions_prob[i][j][1]\n",
    "#         prediction.append(Class)\n",
    "#     return prediction\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(test_y,prediction(test_x,predictions_prob_test))*100\n",
    "# accuracy_score(train_y,prediction(train_x,predictions_prob_train))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
