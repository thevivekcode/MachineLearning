{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"./Alphabets/train.csv\",header=None).to_numpy()\n",
    "test_data = pd.read_csv(\"./Alphabets/test.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:,:-1]/255\n",
    "train_y = train_data[:,-1]\n",
    "test_x = test_data[:,:-1]/255\n",
    "test_y = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training algorithms for deep learning models are usually iterative in nature and thus \n",
    "require the user to specify some initial point from which to begin the iterations. \n",
    "Moreover, training deep models is a sufficiently difficult task that most algorithms are\n",
    "strongly affected by the choice of initialization.\"\"\"\n",
    "\n",
    "class neuralNetwork:\n",
    "    def __init__(self,batchSize,input_features,architecture,target_class,eta,max_iter,activationMode,learningRate):\n",
    "        self.batchSize = batchSize\n",
    "        self.input_features = input_features\n",
    "        self.architecture = architecture\n",
    "        self.target_class = target_class\n",
    "        self.learningRate = learningRate\n",
    "        self.activationMode  = activationMode\n",
    "\n",
    "        self.parameter = []\n",
    "        self.layer_input = [0]*(len(architecture)+1)\n",
    "        self.layer_output = [0]*(len(architecture)+1)\n",
    "        self.layer_delta = [0]*(len(architecture)+1)\n",
    "        self.total_layers = len(architecture)+1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        \n",
    "\n",
    "    def activation(self,x,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif mode==\"relu\":\n",
    "            return np.where(x<0,0,x)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def differentiation(self,op,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return op*(1-op)\n",
    "        elif mode==\"relu\":\n",
    "            return 1 * (op > 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def oneHotEncoding(self,y): #y is **list** of ouput label 0<=y<26\n",
    "        \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "        a = np.array(y)\n",
    "        b = np.zeros((a.size, self.target_class))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return b\n",
    "\n",
    "    \n",
    "\n",
    "    def initalize_parameters(self):\n",
    "\n",
    "        total_layers_architecture = self.architecture  + [self.target_class]\n",
    "        layerInputSize = self.input_features\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        \"\"\"Random intialization is used to preserve the stochastic nature of neural networks\"\"\"\n",
    "        \n",
    "        for layer, total_neurons in enumerate(total_layers_architecture):\n",
    "            np.random.seed(layer)\n",
    "            if layer ==0:\n",
    "                neurons_in_prev = self.input_features\n",
    "            else:\n",
    "                neurons_in_prev = total_layers_architecture[layer-1]\n",
    "                \n",
    "            \"\"\"HE(et.al 2015) INITIALIZATION IS USED TO REMOVE SYMMETRY ANS VANISHING GRADIENT ISSUES\"\"\"  \n",
    "            \n",
    "            layerOutputSize = total_neurons\n",
    "            layer_weight = (np.random.randn(layerOutputSize, layerInputSize)*2)/math.sqrt(neurons_in_prev)\n",
    "            \"\"\"It is important to note that the bias weight in each neuron\n",
    "            is set to zero by default, not a small random value.\"\"\"\n",
    "            layer_bias  = np.zeros((layerOutputSize,1))\n",
    "            \n",
    "            self.parameter.append([layer_weight,layer_bias])\n",
    "\n",
    "            layerInputSize = layerOutputSize\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def full_feedForward(self,X):\n",
    "        current_input = X.copy()\n",
    "\n",
    "        for layer in range(self.total_layers):\n",
    "\n",
    "            current_parameter = self.parameter[layer]\n",
    "\n",
    "            weight = current_parameter[0]\n",
    "            bias = current_parameter[1]\n",
    "            netJ = np.dot(current_input,weight.T) + bias.T\n",
    "            \n",
    "            if self.activationMode == \"relu\":\n",
    "                if layer == self.total_layers-1:\n",
    "                    G_netJ  = self.activation(netJ,\"sigmoid\")   # only output layer \n",
    "                else :\n",
    "                    G_netJ  = self.activation(netJ,\"relu\")   # all hidden layer\n",
    "            \n",
    "            elif self.activationMode == \"sigmoid\":\n",
    "                G_netJ  = self.activation(netJ,\"sigmoid\") # all layers \n",
    "                    \n",
    "\n",
    "            self.layer_output[layer] = G_netJ\n",
    "            self.layer_input[layer] = current_input\n",
    "\n",
    "            current_input = G_netJ.copy()\n",
    "        \"\"\"         last single layer output is the output of entire neural network\n",
    "                     to be used for calculating new loss function value                   \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_backpropagation(self,Y):\n",
    "\n",
    "        \"\"\"  start with last (ouyput)layer whose deltaJ is calculated\n",
    "             differently then rest of hidden layer                    \"\"\"\n",
    "        lastlayer = self.total_layers-1\n",
    "        op = self.layer_output[lastlayer]\n",
    "        \n",
    "        diff_op = self.differentiation(op,\"sigmoid\")  # in every case output layer is sigmoid\n",
    "        \n",
    "        deltaJ_lastlayer = (Y-op)*diff_op/(Y.shape[0])\n",
    "        self.layer_delta[lastlayer] = deltaJ_lastlayer\n",
    "\n",
    "        deltaJ_prev = deltaJ_lastlayer.copy()\n",
    "\n",
    "        #reverse iteration\n",
    "        for layer in range(self.total_layers-1,0,-1):\n",
    "            theta_downNBR  = self.parameter[layer][0] # weight without bias\n",
    "\n",
    "            oj = self.layer_output[layer-1]\n",
    "            \n",
    "            if self.activationMode ==\"relu\":\n",
    "                diff_oj = self.differentiation(oj,\"relu\")\n",
    "            elif self.activationMode==\"sigmoid\":\n",
    "                diff_oj = self.differentiation(oj,\"sigmoid\")\n",
    "                \n",
    "            deltaJ_curr = np.dot(deltaJ_prev, theta_downNBR)*diff_oj\n",
    "\n",
    "            self.layer_delta[layer-1] = deltaJ_curr\n",
    "            deltaJ_prev = deltaJ_curr.copy()\n",
    "            \n",
    "            \n",
    "\n",
    "    def costFunction(self,y):\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return (np.sum((y-final_op)**2))/(2*y.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def updateParameters(self,epochCount):\n",
    "        ETA = self.eta   #defalut for fault tolerance\n",
    "        \n",
    "        if self.learningRate == \"normal\":\n",
    "            ETA = self.eta\n",
    "        elif self.learningRate ==  \"adaptive\":\n",
    "            ETA = self.eta/math.sqrt(epochCount) # as per question requirement \n",
    "            \n",
    "        for i in range(len(self.architecture)+1):\n",
    "            \n",
    "            gradient_W = np.dot(self.layer_delta[i].T, self.layer_input[i])\n",
    "            gradient_B = np.sum(self.layer_delta[i],axis = 0).T.reshape((-1,1))\n",
    "            self.parameter[i][0] = self.parameter[i][0] + (ETA)*gradient_W\n",
    "            self.parameter[i][1] = self.parameter[i][1] + (ETA)*gradient_B\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        self.initalize_parameters()\n",
    "        \n",
    "        indexes = np.arange(x.shape[0])\n",
    "        Y = self.oneHotEncoding(y)\n",
    "        \n",
    "        cost, newcost =0, 0\n",
    "        improvement, old_improvement = math.inf , 0\n",
    "        n_iter_no_change = 20\n",
    "        i = 0\n",
    "        epochNumber = 0\n",
    "        totalBatches = math.ceil(x.shape[0]/self.batchSize)\n",
    "        while(True):\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "            \"\"\"shuffle the data after every_epoch to maintain stochastic nature(random) of the newtork\"\"\"\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "            epochNumber = i\n",
    "            \n",
    "            for j in range(0,x.shape[0],self.batchSize):\n",
    "                \n",
    "                batch = indexes[j:j + self.batchSize]\n",
    "                \n",
    "                x_batch = x[batch]\n",
    "                y_batch = Y[batch]\n",
    "\n",
    "                self.full_feedForward(x_batch)\n",
    "               \n",
    "                self.full_backpropagation(y_batch)\n",
    "\n",
    "                self.updateParameters(epochNumber)\n",
    "                \n",
    "                cost += self.costFunction(y_batch)\n",
    "\n",
    "            oldcost = newcost\n",
    "            newcost = cost/totalBatches\n",
    "            cost = 0\n",
    "            \n",
    "            old_improvement = improvement\n",
    "            improvement  = abs(oldcost - newcost)\n",
    "            \n",
    "            if improvement <= 1e-6:\n",
    "                print(\"convergence reached with total epoch :\",i)\n",
    "                return i\n",
    "                break\n",
    "            \n",
    "            if abs(improvement - old_improvement) < 1e-6:\n",
    "                n_iter_no_change -= 1\n",
    "            \n",
    "            #early stopping\n",
    "            if n_iter_no_change ==0:\n",
    "                print(\"Converged after cost not improved for n_iter_no_change \",i)\n",
    "                break\n",
    "                \n",
    "            if i == self.max_iter:\n",
    "                print(\"max_iter reached\")\n",
    "                return i\n",
    "                break   \n",
    "                \n",
    "            if i%100 ==0:\n",
    "                print(improvement)\n",
    "                print('Current Epoch is : ',i)\n",
    "                \n",
    "                \n",
    "\n",
    "    def score(self,x,y):\n",
    "        self.full_feedForward(x)\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return np.count_nonzero((np.argmax(final_op,axis =1) == y) == True)/y.shape[0] #,np.argmax(final_op,axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchSize| input_features| architecture| target_class| eta,max_iter| activationMode| learningRate\n",
    "model = neuralNetwork(100,784,[5],26,0.5,3000,\"sigmoid\",\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000316585770587241\n",
      "Current Epoch is :  100\n",
      "0.0004337900719039167\n",
      "Current Epoch is :  200\n",
      "0.0002100065541764673\n",
      "Current Epoch is :  300\n",
      "0.00016918404887705796\n",
      "Current Epoch is :  400\n",
      "7.345551970266984e-05\n",
      "Current Epoch is :  500\n",
      "1.4370972459765152e-05\n",
      "Current Epoch is :  600\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "model.fit(train_x,train_y)\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08838461538461538"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08846153846153847"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(y,targetClass): #y is **list** of ouput label 0<=y<26\n",
    "    \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "    a = np.array(y)\n",
    "    b = np.zeros((a.size,targetClass ))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = oneHotEncoding(train_y,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when i thought we need to train 26 models for each class as binary classification \n",
    "\n",
    "# models = []\n",
    "# for i in tqdm(range(26)):\n",
    "#     model = MLPClassifier(hidden_layer_sizes=(100,100,), activation='relu', solver='lbfgs', \n",
    "#                       alpha=0.0001, batch_size=100, learning_rate='adaptive', \n",
    "#                       learning_rate_init=0.5, power_t=0.5, max_iter=200, \n",
    "#                       shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "#                       warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "#                       validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "#                       n_iter_no_change=10, max_fun=15000)\n",
    "#     model.fit(train_x,Y[:,i])\n",
    "#     models.append(model)\n",
    "    \n",
    "# predictions_prob_train = []\n",
    "# predictions_prob_test = []\n",
    "# for i in range(26):\n",
    "#     model  = models[i]\n",
    "#     predictions_prob_train.append(model.predict_proba(train_x))\n",
    "#     predictions_prob_test.append(model.predict_proba(test_x))\n",
    "\n",
    "    \n",
    "# def prediction(x,predictions_prob)\n",
    "#     prediction= []\n",
    "#     for j in range(x.shape[0]):\n",
    "#         prob = 0\n",
    "#         Class = -1\n",
    "#         for i in range(26):\n",
    "\n",
    "#             if prob < predictions_prob[i][j][1]:\n",
    "#                 Class = i\n",
    "#                 prob = predictions_prob[i][j][1]\n",
    "#         prediction.append(Class)\n",
    "#     return prediction\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(test_y,prediction(test_x,predictions_prob_test))*100\n",
    "# accuracy_score(train_y,prediction(train_x,predictions_prob_train))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0, batch_size=100, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
       "              learning_rate_init=0.1, max_fun=15000, max_iter=2000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making aplha =0 as we need to compare with our results which include no regularizer\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,100), activation='relu', solver='sgd', \n",
    "                      alpha=0, batch_size=100, learning_rate='invscaling', \n",
    "                      learning_rate_init=0.1, power_t=0.5, max_iter=2000, \n",
    "                      shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "                      warm_start=False, momentum=0.9, nesterovs_momentum=True, n_iter_no_change=10)\n",
    "model.fit(train_x,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_train = model.predict_proba(train_x)\n",
    "prob_test = model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.35384615384615\n",
      "87.03076923076924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_y,prob_train.argmax(axis = 1))*100)\n",
    "print(accuracy_score(test_y,prob_test.argmax(axis = 1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
