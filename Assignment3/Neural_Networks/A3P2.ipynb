{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"./Alphabets/train.csv\",header=None).to_numpy()\n",
    "test_data = pd.read_csv(\"./Alphabets/test.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data[:,:-1].astype(\"float64\")/255\n",
    "train_y = train_data[:,-1]\n",
    "test_x = test_data[:,:-1].astype(\"float64\")/255\n",
    "test_y = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self,batchSize,input_features,architecture,target_class,eta,epoch,activationMode,learningRate):\n",
    "        self.batchSize = batchSize\n",
    "        self.input_features = input_features\n",
    "        self.architecture = architecture\n",
    "        self.target_class = target_class\n",
    "        self.learningRate = learningRate\n",
    "        self.activationMode  = activationMode\n",
    "\n",
    "        self.parameter = []\n",
    "        self.layer_input = [0]*(len(architecture)+1)\n",
    "        self.layer_output = [0]*(len(architecture)+1)\n",
    "        self.layer_delta = [0]*(len(architecture)+1)\n",
    "        self.total_layers = len(architecture)+1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "\n",
    "\n",
    "    def activation(self,x,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif mode==\"relu\":\n",
    "            return np.maximum(0,x)\n",
    "        \n",
    "        \n",
    "    def differentiation(self,op,mode):\n",
    "        if mode==\"sigmoid\":\n",
    "            return op*(1-op)\n",
    "        elif mode==\"relu\":\n",
    "            return 1 * (op > 0)\n",
    "        \n",
    "        \n",
    "    def oneHotEncoding(self,y): #y is **list** of ouput label 0<=y<26\n",
    "        \"\"\" converts y label to vector representation called One Hot Encoding \"\"\"\n",
    "        a = np.array(y)\n",
    "        b = np.zeros((a.size, self.target_class))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        return b\n",
    "\n",
    "\n",
    "    def initalize_parameters(self):\n",
    "\n",
    "        total_layers_architecture = self.architecture  + [self.target_class]\n",
    "        layerInputSize = self.input_features\n",
    "        np.random.seed(0)\n",
    "        \"\"\"first hidden layer starts with 0 \"\"\"\n",
    "        for layer, total_neurons in enumerate(total_layers_architecture):\n",
    "\n",
    "            layerOutputSize = total_neurons\n",
    "\n",
    "            layer_weight = np.random.uniform(low=-0.3, high=0.3, size=(layerOutputSize, layerInputSize))\n",
    "            layer_bias  = np.random.uniform(low=-0.3, high=0.3, size=(layerOutputSize,1))\n",
    "\n",
    "            self.parameter.append([layer_weight,layer_bias])\n",
    "\n",
    "            layerInputSize = layerOutputSize\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def full_feedForward(self,X):\n",
    "        current_input = X.copy()\n",
    "\n",
    "        for layer in range(self.total_layers):\n",
    "\n",
    "            current_parameter = self.parameter[layer]\n",
    "\n",
    "            weight = current_parameter[0]\n",
    "            bias = current_parameter[1]\n",
    "            netJ = np.dot(current_input,weight.T) + bias.T\n",
    "            \n",
    "            if self.activationMode == \"relu\":\n",
    "                if layer == self.total_layers-1:\n",
    "                    G_netJ  = self.activation(netJ,\"sigmoid\")   # only output layer \n",
    "                else :\n",
    "                    G_netJ  = self.activation(netJ,\"relu\")   # all hidden layer\n",
    "            \n",
    "            elif self.activationMode == \"sigmoid\":\n",
    "                G_netJ  = self.activation(netJ,\"sigmoid\") # all layers \n",
    "                    \n",
    "\n",
    "            self.layer_output[layer] = G_netJ\n",
    "            self.layer_input[layer] = current_input\n",
    "\n",
    "            current_input = G_netJ.copy()\n",
    "        \"\"\"         last single layer output is the output of entire neural network\n",
    "                     to be used for calculating new loss function value                   \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_backpropagation(self,Y):\n",
    "\n",
    "        \"\"\"  start with last (ouyput)layer whose deltaJ is calculated\n",
    "             differently then rest of hidden layer                    \"\"\"\n",
    "        lastlayer = self.total_layers-1\n",
    "        op = self.layer_output[lastlayer]\n",
    "        \n",
    "        diff_op = self.differentiation(op,\"sigmoid\")  # in every case output layer is sigmoid\n",
    "        \n",
    "        deltaJ_lastlayer = (Y-op)*diff_op/(Y.shape[0])\n",
    "        self.layer_delta[lastlayer] = deltaJ_lastlayer\n",
    "\n",
    "        deltaJ_prev = deltaJ_lastlayer.copy()\n",
    "\n",
    "        #reverse iteration\n",
    "        for layer in range(self.total_layers-1,0,-1):\n",
    "            theta_downNBR  = self.parameter[layer][0] # weight without bias\n",
    "\n",
    "            oj = self.layer_output[layer-1]\n",
    "            \n",
    "            if self.activationMode ==\"relu\":\n",
    "                diff_oj = self.differentiation(oj,\"relu\")\n",
    "            elif self.activationMode==\"sigmoid\":\n",
    "                diff_oj = self.differentiation(oj,\"sigmoid\")\n",
    "                \n",
    "            deltaJ_curr = np.dot(deltaJ_prev, theta_downNBR)*diff_oj\n",
    "\n",
    "            self.layer_delta[layer-1] = deltaJ_curr\n",
    "            deltaJ_prev = deltaJ_curr.copy()\n",
    "\n",
    "    def costFunction(self,y):\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return (np.sum((y-final_op)**2))/(2*y.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def updateParameters(self,epochCount):\n",
    "        ETA = self.eta\n",
    "        \n",
    "        if self.learningRate == \"normal\":\n",
    "            ETA = self.eta\n",
    "        elif self.learningRate ==  \"adaptive\":\n",
    "            ETA = self.eta/math.sqrt(epochCount)\n",
    "            \n",
    "        for i in range(len(self.architecture)+1):\n",
    "            \n",
    "            gradient_W = np.dot(self.layer_delta[i].T, self.layer_input[i])\n",
    "            gradient_B = np.sum(self.layer_delta[i],axis = 0).T.reshape((-1,1))\n",
    "            self.parameter[i][0] = self.parameter[i][0] + (ETA)*gradient_W\n",
    "            self.parameter[i][1] = self.parameter[i][1] + (ETA)*gradient_B\n",
    "\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        self.initalize_parameters()\n",
    "        cost, newcost = 0,0\n",
    "\n",
    "        Y = self.oneHotEncoding(y)\n",
    "        i = 1\n",
    "        epochNumber = 1\n",
    "#         while(True):\n",
    "        for i in range(self.epoch):\n",
    "            if i%10 ==0:\n",
    "                epochNumber  = i\n",
    "            totalBatches = math.ceil(x.shape[0]/self.batchSize)\n",
    "            for j in range(0,x.shape[0],self.batchSize):\n",
    "                \n",
    "                x_batch = x[j:j+self.batchSize]\n",
    "                y_batch = Y[j:j+self.batchSize]\n",
    "\n",
    "                self.full_feedForward(x_batch)\n",
    "               \n",
    "                self.full_backpropagation(y_batch)\n",
    "\n",
    "                self.updateParameters(epochNumber)\n",
    "#                 cost += self.costFunction(y_batch)\n",
    "#             if i%200==0:\n",
    "#                 print(\"currentEpoch\",i)\n",
    "#                 oldcost = newcost\n",
    "#                 newcost = cost/200\n",
    "#                 cost = 0\n",
    "#                 print(\"differnce is \",abs(oldcost - newcost))\n",
    "#                 if abs(oldcost - newcost) <= 1e-4:\n",
    "#                     print(\"stopping as convergence achieved \",)\n",
    "#                     print('total epoch is :', i)\n",
    "#                     break\n",
    "                \n",
    "#             i+=1\n",
    "\n",
    "\n",
    "\n",
    "    def score(self,x,y):\n",
    "        self.full_feedForward(x)\n",
    "        final_op = self.layer_output[self.total_layers-1]\n",
    "        return np.count_nonzero((np.argmax(final_op,axis =1) == y) == True)/y.shape[0],np.argmax(final_op,axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchSize,input_features,architecture,target_class,eta,epoch,learningRate\n",
    "model = neuralNetwork(100,784,[100],26,0.1,1000,\"sigmoid\",\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.6145706176758\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "model.fit(train_x,train_y)\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9473846153846154, array([24,  5, 24, ...,  7, 15, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8943076923076924, array([25, 13,  6, ...,  3,  4, 12]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
