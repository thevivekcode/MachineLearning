{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UAkRy_aqvOnI",
    "outputId": "6f4329ff-4551-4025-8e7f-ef1b3e67eff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1VXXIxm82rQk",
    "outputId": "e6c07c8a-6537-4181-ec71-1521d34407ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.4.13-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 425 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.0.0 sacrebleu-1.4.13\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/pratik/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/pratik/anaconda3/lib/python3.7/site-packages (from spacy) (1.18.1)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/pratik/anaconda3/lib/python3.7/site-packages (from spacy) (4.42.1)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/pratik/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/pratik/anaconda3/lib/python3.7/site-packages (from spacy) (49.1.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/pratik/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/pratik/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/pratik/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/pratik/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pratik/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/pratik/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Installing collected packages: cymem, murmurhash, preshed, wasabi, blis, catalogue, plac, srsly, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/pratik/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sacrebleu\n",
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtJG0fr9xMhF"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FYUpld8evZfj",
    "outputId": "fb72b157-08e3-4fb1-949b-df0eb6ce0674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('it_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as Tmodels\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "import spacy.cli\n",
    "import spacy  # for tokenizer\n",
    "import os\n",
    "import csv\n",
    "import sacrebleu\n",
    "spacy.cli.download(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RgzcUnTxJmt"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og8TeGchvcRn"
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = 100\n",
    "BEAM=5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqh1CtkMxGEt"
   },
   "source": [
    "# Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaTUrefMvdz7"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "  \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "  Args:\n",
    "      output_size (tuple or int): Desired output size. If tuple, output is\n",
    "          matched to output_size. If int, smaller of image edges is matched\n",
    "          to output_size keeping aspect ratio the same.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, output_size):\n",
    "    assert isinstance(output_size, (int, tuple))\n",
    "    self.output_size = output_size\n",
    "\n",
    "  def __call__(self, image):\n",
    "    h, w = image.shape[:2]\n",
    "    if isinstance(self.output_size, int):\n",
    "        if h > w:\n",
    "            new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        else:\n",
    "            new_h, new_w = self.output_size, self.output_size * w / h\n",
    "    else:\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "    new_h, new_w = int(new_h), int(new_w)\n",
    "    img = transform.resize(image, (new_h, new_w))\n",
    "    return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "  \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "  def __call__(self, image):\n",
    "    # swap color axis because\n",
    "    # numpy image: H x W x C\n",
    "    # torch image: C X H X W\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMJK2nxOw_Os"
   },
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPnTGrosvrjX"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "  def __init__(self):\n",
    "    self.freq = {\"<PAD>\": 0, \"<SOS>\": 0, \"<EOS>\": 0, \"<UNK>\": 0}\n",
    "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    self.idx = 4\n",
    "    \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.itos)\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer(text):\n",
    "    tokens =  [tok.text.lower() for tok in sp(text)]\n",
    "    return tokens\n",
    "\n",
    "  def build_vocabulary(self, sentence_list):\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "      for word in self.tokenizer(sentence):\n",
    "        if word not in self.freq:\n",
    "          self.freq[word] = 1\n",
    "\n",
    "        else:\n",
    "          self.freq[word] += 1\n",
    "\n",
    "        if self.freq[word] == 1:\n",
    "          self.stoi[word] = self.idx\n",
    "          self.itos[self.idx] = word\n",
    "          self.idx += 1\n",
    "\n",
    "  def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer(text)\n",
    "\n",
    "    return [\n",
    "      self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "      for token in tokenized_text\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8GF0Y_Pw7Aj"
   },
   "source": [
    "# Captions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-jxo5ASwtHO"
   },
   "outputs": [],
   "source": [
    "class CaptionsPreprocessing:\n",
    "  \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "\n",
    "  Args:\n",
    "      captions_file_path (string): captions tsv file path\n",
    "  \"\"\"\n",
    "  def __init__(self, captions_file_path):\n",
    "    self.captions_file_path = captions_file_path\n",
    "\n",
    "    # Read raw captions\n",
    "    self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "    # Preprocess captions\n",
    "    self.captions_dict = self.process_captions()\n",
    "\n",
    "    # Create vocabulary\n",
    "    self.vocab =  Vocabulary()\n",
    "    self.generate_vocabulary()\n",
    "\n",
    "  def read_raw_captions(self):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Dictionary with raw captions list keyed by image ids (integers)\n",
    "    \"\"\"\n",
    "\n",
    "    captions_dict = {}\n",
    "    with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "      for img_caption_line in f.readlines():\n",
    "        img_captions = img_caption_line.strip().split('\\t')\n",
    "        captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "    return captions_dict\n",
    "\n",
    "  def process_captions(self):\n",
    "    \"\"\"\n",
    "    Use this function to generate dictionary and other preprocessing on captions\n",
    "    \"\"\"\n",
    "\n",
    "    raw_captions_dict = self.raw_captions_dict\n",
    "\n",
    "    # Do the preprocessing here\n",
    "    captions_dict = raw_captions_dict\n",
    "\n",
    "    return captions_dict\n",
    "\n",
    "  def generate_vocabulary(self):\n",
    "    \"\"\"\n",
    "    Use this function to generate dictionary and other preprocessing on captions\n",
    "    \"\"\"\n",
    "\n",
    "    captions_dict = self.captions_dict\n",
    "\n",
    "    # Generate the vocabulary\n",
    "\n",
    "    for captionList in captions_dict.values():\n",
    "      self.vocab.build_vocabulary(captionList)\n",
    "\n",
    "    \n",
    "\n",
    "  def captions_transform(self, img_caption_list):\n",
    "    \"\"\"\n",
    "    Use this function to generate tensor tokens for the text captions\n",
    "    Args:\n",
    "        img_caption_list: List of captions for a particular image\n",
    "    \"\"\"\n",
    "    vocab = self.vocab\n",
    "\n",
    "    # Generate tensors\n",
    "\n",
    "    img_caption_List= img_caption_list.copy()\n",
    "    captions_length = []\n",
    "    for index, caption in enumerate(img_caption_list):\n",
    "      numericalized_caption  = [self.vocab.stoi[\"<SOS>\"]]\n",
    "      numericalized_caption += self.vocab.numericalize(caption)\n",
    "      numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "      captions_length.append(len(numericalized_caption))\n",
    "      img_caption_List[index] = numericalized_caption\n",
    "\n",
    "    img_caption_matrix  = np.zeros([len(img_caption_List),MAX_TEXT_LENGTH])\n",
    "    for i,j in enumerate(img_caption_List):\n",
    "      img_caption_matrix[i][0:len(j)] = j\n",
    "\n",
    "    caption_tensor =  torch.from_numpy(img_caption_matrix)\n",
    "    caption_tensor = caption_tensor.long()\n",
    "\n",
    "    return caption_tensor, torch.tensor(captions_length).int()\n",
    "\n",
    "# Set the captions tsv file path\n",
    "CAPTIONS_FILE_PATH = '/home/pratik/Downloads/akshaymodels/train_captions.tsv'\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)\n",
    "\n",
    "# TEST_CAPTIONS_FILE_PATH = 'drive/My Drive/Colab Notebooks/CNN/test_captions.tsv'\n",
    "# test_captions_preprocessing_obj = CaptionsPreprocessing(TEST_CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WcUj3jGJw3g0"
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRvrgJa4wzF7"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "  def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_dir (string): Directory with all the images.\n",
    "        captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "        img_transform (callable, optional): Optional transform to be applied\n",
    "            on the image sample.\n",
    "\n",
    "        captions_transform: (callable, optional): Optional transform to be applied\n",
    "            on the caption sample (list).\n",
    "    \"\"\"\n",
    "    self.img_dir = img_dir\n",
    "    self.captions_dict = captions_dict\n",
    "    self.img_transform = img_transform\n",
    "    self.captions_transform = captions_transform\n",
    "\n",
    "    self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "    image = io.imread(img_name)\n",
    "    captions = self.captions_dict[self.image_ids[idx]]\n",
    "\n",
    "    if self.img_transform:\n",
    "        image = self.img_transform(image)\n",
    "\n",
    "    if self.captions_transform:\n",
    "        captions, captions_length = self.captions_transform(captions)\n",
    "\n",
    "    sample = {'id': idx, 'image': image, 'captions': captions, 'captions_length': captions_length}\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARuX90c9w2TZ"
   },
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPaJaYj-xXiv"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"/home/pratik/Downloads/akshaymodels/model.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9FgBeAJxgNW"
   },
   "source": [
    "# Encoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2HqxG8hxjLh"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "  def __init__(self, embed_size):\n",
    "    \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "    super(EncoderCNN, self).__init__()\n",
    "    resnet = resnet50(pretrained=True)\n",
    "    modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "    self.resnet = nn.Sequential(*modules)\n",
    "    self.modules__ = resnet\n",
    "    self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "    \n",
    "  def forward(self, images):\n",
    "    \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "    with torch.no_grad():\n",
    "      features = self.resnet(images.type(torch.cuda.FloatTensor))\n",
    "      features = features.reshape(features.size(0), -1)\n",
    "    features = self.bn(F.relu(self.linear(features)))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlFwzEC26y2E"
   },
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSTdqczpzViO"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    \n",
    "    # define the properties\n",
    "    self.embed_size = embed_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.lstm = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    self.linear = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "    self.l2 = nn.Linear(1, self.hidden_size)\n",
    "\n",
    "\n",
    "  def forward(self, features, captions, captions_length):\n",
    "    captions_length, sort_ind = captions_length.sort(dim=0, descending=True)\n",
    "    captions = captions[sort_ind]\n",
    "    features = features[sort_ind]\n",
    "    batch_size = features.size(0)\n",
    "    feature_mean = torch.mean(features, dim = 1)\n",
    "    \n",
    "    hidden_state = self.l2(feature_mean.unsqueeze(1))\n",
    "    [hidden_state, cell_state] = [hidden_state.to(device)]*2\n",
    "\n",
    "    outputs = torch.zeros((batch_size, captions.size(1), self.vocab_size)).to(device)\n",
    "    decode_lengths = (captions_length).tolist()\n",
    "\n",
    "    # decoder is fed encoding as well as captions except <EOS> \n",
    "    for t in range(max(decode_lengths)):\n",
    "      batch_size_t = sum([l > t for l in decode_lengths])\n",
    "      if t == 0:\n",
    "        captions_embed = features\n",
    "      else:\n",
    "        captions_embed = self.embed(captions[:batch_size_t,t-1])\n",
    "\n",
    "      hidden_state, cell_state = self.lstm(captions_embed, (hidden_state[:batch_size_t], cell_state[:batch_size_t]))\n",
    "      hidden_state = self.dropout(hidden_state)\n",
    "      out = self.linear(hidden_state)\n",
    "      outputs[:batch_size_t, t, :] = out  # contains <SOS> ---- <EOS>\n",
    "    return outputs, captions, sort_ind, captions_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3j3PIxpzrme"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9bdfERt8xD3"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "    super(ImageCaptionsNet, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.encoderCNN = EncoderCNN(embed_size)\n",
    "    self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "\n",
    "  def forward(self,  images, captions, captions_length):\n",
    "    features = self.encoderCNN(images)\n",
    "    features = torch.repeat_interleave(features,dim = 0,repeats = 5)\n",
    "    outputs, captions, sort_id, decode_lengths = self.decoderRNN(features, captions, captions_length)\n",
    "    return outputs, captions, sort_id, decode_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHeHO4TC-BQM"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_j8NZY9-EaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = '/home/pratik/Downloads/akshaymodels/1/'\n",
    "# IMAGE_TEST_DIR = 'drive/My Drive/Colab Notebooks/CNN/test_images/1/'\n",
    "# PRIVATE_IMAGE_TEST_DIR = 'drive/My Drive/Colab Notebooks/CNN/private_test_images/1/'\n",
    "\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "\n",
    "# # Creating the test Dataset\n",
    "# test_dataset = ImageCaptionsDataset(\n",
    "#     IMAGE_TEST_DIR, test_captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "#     captions_transform=test_captions_preprocessing_obj.captions_transform\n",
    "# )\n",
    "\n",
    "# # Creating the test Dataset\n",
    "# private_test_dataset = ImageCaptionsDataset(\n",
    "#     PRIVATE_IMAGE_TEST_DIR, test_captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "#     captions_transform=test_captions_preprocessing_obj.captions_transform\n",
    "# )\n",
    "\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "embed_size = 1024\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab_size = len(captions_preprocessing_obj.vocab) # to be changed for training\n",
    "NUM_WORKERS = 4 # Parallel threads for dataloading\n",
    "\n",
    "\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "\n",
    "net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "\n",
    "# assign class weight to each vocab index\n",
    "class_weights = torch.ones(vocab_size).to(device)\n",
    "class_weights[captions_preprocessing_obj.vocab.stoi[\"<PAD>\"]] = 0  # ignore pading loss\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "params = list(net.encoderCNN.parameters()) + list(net.decoderRNN.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params = params, lr=LEARNING_RATE, weight_decay = 0)\n",
    "\n",
    "step = 0\n",
    "\n",
    "if load_model:\n",
    "  step = load_checkpoint(torch.load(\"/home/pratik/Downloads/akshaymodels/model.tar\"), net, optimizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0uQbsEHKzP0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Iteration: 10\n",
      "loss tensor(2.4988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 11\n",
      "loss tensor(2.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 12\n",
      "loss tensor(2.5595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 13\n",
      "loss tensor(2.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 14\n",
      "loss tensor(2.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 15\n",
      "loss tensor(2.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 16\n",
      "loss tensor(2.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 17\n",
      "loss tensor(2.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 18\n",
      "loss tensor(2.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 19\n",
      "loss tensor(2.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "=> Saving checkpoint\n",
      "Iteration: 20\n",
      "loss tensor(1.9447, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af8787ee7228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcaptions_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c735ae135c08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, captions, captions_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5d1d9541df45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, captions, captions_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdecode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcaptions_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "\n",
    "# sample = next(iter(train_loader))\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "\n",
    "  for batch_idx, sample in enumerate(train_loader):\n",
    "    image_batch, captions_batch, captions_length = (sample['image']/255).to(device), sample['captions'].to(device), sample['captions_length'].to(device)\n",
    "\n",
    "    caption_batch = caption_batch.view(-1,captions_batch.shape[2])\n",
    "    captions_length = captions_length.view(-1)\n",
    "    \n",
    "    \n",
    "    outputs, captions, sort_id, decode_lengths = net(image_batch, captions_batch, captions_length)\n",
    "    \n",
    "    loss = loss_function(outputs.reshape(-1,outputs.shape[2]), captions.reshape(-1))\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  if save_model:\n",
    "    checkpoint = { \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict(), \"step\": step }\n",
    "    save_checkpoint(checkpoint)\n",
    "  step+=1\n",
    "  print(\"Iteration: \" + str(step))\n",
    "  print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKGDagm5BTHe"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGxULt9eB8Vr"
   },
   "outputs": [],
   "source": [
    "def temp_pred(model, image, vocabulary):\n",
    "  result_caption = []\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    input = model.encoderCNN(image)\n",
    "\n",
    "    feature_mean = torch.mean(input, dim = 1)\n",
    "    hidden_state = model.decoderRNN.l2(feature_mean.unsqueeze(1))\n",
    "    [hidden_state, cell_state] = [hidden_state.to(device)]*2\n",
    "\n",
    "    hidden  = torch.zeros((1,model.hidden_size)).to(device)\n",
    "    cell = torch.zeros((1,model.hidden_size)).to(device)\n",
    "\n",
    "    for i in range(MAX_TEXT_LENGTH):\n",
    "      hiddens, cell = model.decoderRNN.lstm(input, (hidden, cell))\n",
    "      output = model.decoderRNN.linear(hiddens)\n",
    "      pred = output.argmax(1)\n",
    "      result_caption.append(pred)\n",
    "      input = model.decoderRNN.embed(pred)\n",
    "      if vocabulary.itos[pred.item()] == \"<EOS>\" or vocabulary.itos[pred.item()] == \"<PAD>\" :\n",
    "        break\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "  return [vocabulary.itos[i.item()] for i in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrPFWvBeETfD"
   },
   "outputs": [],
   "source": [
    "IMAGE_TEST_DIR = '/home/pratik/Downloads/akshaymodels/test_images/1/'\n",
    "with open('2019MCS2574_2019MCS2556_test.tsv', 'w+') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    files = os.listdir(IMAGE_TEST_DIR)\n",
    "    for f in files:\n",
    "\n",
    "        image = io.imread(IMAGE_TEST_DIR+f)\n",
    "        image = torch.tensor(img_transform(image))\n",
    "        # plt.imshow(image.permute(1,2,0))\n",
    "        # break\n",
    "        res = temp_pred(net,image.unsqueeze(0),captions_preprocessing_obj.vocab)\n",
    "        st_fake = ' '.join([i for i in res if i != '<SOS>' and i != '<EOS>' and i != '<PAD>'])\n",
    "        id = f.split(\"_\")[-1]\n",
    "        id = id.split(\".\")[0]\n",
    "        tsv_writer.writerow([id, st_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MM_72M7P4nbF"
   },
   "outputs": [],
   "source": [
    "PRIVATE_IMAGE_TEST_DIR = '/home/pratik/Downloads/akshaymodels/private_test_images/'\n",
    "with open('2019MCS2574_2019MCS2556_private.tsv', 'w+') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    files = os.listdir(PRIVATE_IMAGE_TEST_DIR)\n",
    "    for f in files:\n",
    "\n",
    "        image = io.imread(PRIVATE_IMAGE_TEST_DIR+f)\n",
    "        image = torch.tensor(img_transform(image))\n",
    "        # plt.imshow(image.permute(1,2,0))\n",
    "        # break\n",
    "        res = temp_pred(net,image.unsqueeze(0),captions_preprocessing_obj.vocab)\n",
    "        st_fake = ' '.join([i for i in res if i != '<SOS>' and i != '<EOS>' and i != '<PAD>'])\n",
    "        id = f.split(\"_\")[-1]\n",
    "        id = id.split(\".\")[0]\n",
    "        tsv_writer.writerow([id, st_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeQdJUCe5NwI"
   },
   "outputs": [],
   "source": [
    "# with open('2019MCS2556_2019MCS2574_public.tsv', 'wt') as out_file:\n",
    "#   tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "#   for sample in test_loader:\n",
    "#     image_batch, ids, caption_batch = sample['image']/255, sample['id'], sample['captions']\n",
    "#     for image, id, caption in zip(image_batch, ids, caption_batch):\n",
    "#       res = temp_pred(net,image.unsqueeze(0),captions_preprocessing_obj.vocab)\n",
    "#       st_fake = ' '.join([i \n",
    "#                           for i in res if i != '<SOS>' and i != '<EOS>' and i != '<PAD>'])\n",
    "#       tsv_writer.writerow([id.item(), st_fake])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "comp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
