{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Stopping and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_stemming(df):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Split the sentences to lists of words.\n",
    "    df['split'] = df['B'].str.strip().str.lower().str.replace(\",\",\" \").str.replace(\".\",\" \").str.replace(\"!\",\"\").str.split()\n",
    "    df = df.drop(columns=['B']) # Get rid of the old column.\n",
    "        \n",
    "    df['stopped_stemmed'] = df['split'].apply(lambda x: [stemmer.stem(item) for item in x if item not in stops and not item.startswith('@')])\n",
    "    df = df.drop(columns=['split']) # Get rid of the old column.\n",
    "    \n",
    "    df['B'] =  df['stopped_stemmed'].apply(lambda x: \" \".join(x))\n",
    "    df = df.drop(columns=['stopped_stemmed']) # Get rid of the old column.\n",
    "#     return  df.to_numpy()\n",
    "    return df \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input: training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTraining(case):\n",
    "    df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    if case == 1:\n",
    "        return df.to_numpy()\n",
    "    elif case == 2:\n",
    "        return stopword_stemming(df)\n",
    "    \n",
    "\n",
    "def inputTesting():\n",
    "    df = pd.read_csv(\"./trainingandtestdata/testdata.manual.2009.06.14.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    data = df.to_numpy()\n",
    "    data = data[np.where(data[:,0]!=2)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = Polarity\n",
    "# B = TweetID\n",
    "# C = date of the tweet \n",
    "# D = query (lyx)\n",
    "# E = user\n",
    "# F = tweet\n",
    "\n",
    "#-----------CLASS------------\n",
    "\n",
    "#class 0 Positive \n",
    "#calss 4 negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary of all words in training data\n",
    "\n",
    "def vocabulary(data):\n",
    "    data_class0 = data[np.where(data[:,0]==0)]\n",
    "#     data_class2 = data[np.where(data[:,0]==2)]\n",
    "    data_class4 = data[np.where(data[:,0]==4)]\n",
    "    dataClassList  = [data_class0,data_class4]\n",
    "    vocab_class0 = dict()\n",
    "    vocab_class2 = dict()\n",
    "    vocab_class4 = dict()\n",
    "    TotalVocab = dict()\n",
    "    wc = []\n",
    "    vocabClassList = [vocab_class0,vocab_class4]\n",
    "    for i,j in zip(dataClassList,vocabClassList):\n",
    "        totalWordCountInClass = 0\n",
    "        tweetList = i[:,1]\n",
    "        for tweet in tweetList:\n",
    "            tweetlist = tweet.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "#             tweetlist = tweet.split()\n",
    "            for words in tweetlist:\n",
    "                totalWordCountInClass+=1\n",
    "                if words in j:\n",
    "                    j[words]+=1\n",
    "                    TotalVocab[words]+=1\n",
    "                else:\n",
    "                    j[words]=1\n",
    "                    TotalVocab[words]=1\n",
    "        wc.append(totalWordCountInClass)\n",
    "    print(\"Vocalbulary done\")\n",
    "    return vocab_class0,vocab_class4, TotalVocab,wc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the paarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding parameters\n",
    "\n",
    "def FindPhi(data):\n",
    "    phi = np.zeros((2,1))\n",
    "    phi[0,0] = (np.count_nonzero(data[:,0] == 0)+1)/(data.shape[0]+2)\n",
    "#     phi[1,0] = (np.count_nonzero(data[:,0] == 2)+1)/(data.shape[0]+3)\n",
    "    phi[1,0] = (np.count_nonzero(data[:,0] == 4)+1)/(data.shape[0]+2)\n",
    "    return phi\n",
    "\n",
    "def Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc):\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    theta = dict()\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    for words in TotalVocab:\n",
    "        theta[words] = []\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class0:\n",
    "                theta_ofThe_Word = (vocab_class0[words] + 1)/(wc[0] + len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[0] + len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "   \n",
    "#     for words in TotalVocab:\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class4:\n",
    "                theta_ofThe_Word = (vocab_class4[words] + 1)/(wc[1]+ len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[1]+ len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    print(\"parameters calculated\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(TestingData,theta,phi,case,test):\n",
    "    prediction = []\n",
    "    for tweet in TestingData[:,1]:\n",
    "        class0 = phi[0,0]\n",
    "        class4 = phi[1,0]\n",
    "        \n",
    "        #TESTING DATA\n",
    "        if test == 0:\n",
    "            if case ==1:\n",
    "                \n",
    "                tweetlist = tweet.replace(',',\" \").replace('.',\" \").split()\n",
    "            \n",
    "            elif case ==2:\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "                tweetlist =  [stemmer.stem(item) for item in tweet.strip().lower().replace(\"!\",\"\").replace(\",\",\" \").replace(\".\",\" \").split() if item not in stops and not item.startswith('@')]\n",
    "       \n",
    "        # TRAINING DATA\n",
    "        if test == 1:\n",
    "            tweetlist = tweet.split()\n",
    "         \n",
    "        #CALCULATE PROBABILTY\n",
    "        for words in tweetlist:\n",
    "            if words in theta:\n",
    "                class0 += math.log(theta[words][0]) \n",
    "            else:\n",
    "                class0 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "            \n",
    "            if words in theta:\n",
    "                class4 += math.log(theta[words][1]) \n",
    "            else :\n",
    "                class4 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "        \n",
    "        class0 += math.log(phi[0,0])\n",
    "        class4 += math.log(phi[1,0])\n",
    "        if class0 > class4:\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(4)\n",
    "    count =0\n",
    "    correct_class0 = 0\n",
    "    correct_class4 = 0\n",
    "    incorrect_class0 = 0\n",
    "    incorrect_class4 = 0\n",
    "    for i in  range(TestingData.shape[0]):\n",
    "        if prediction[i] == TestingData[i,0]:\n",
    "            if prediction[i] == 0:\n",
    "                correct_class0+=1\n",
    "            else:\n",
    "                correct_class4+=1\n",
    "            count+=1\n",
    "        else:\n",
    "            if prediction[i]==0:\n",
    "                incorrect_class4+=1\n",
    "            else:\n",
    "                incorrect_class0+=1\n",
    "    \n",
    "    confusionMatrix = np.array([[correct_class0,incorrect_class4],[incorrect_class0,correct_class4]])\n",
    "    Accurarcy = (count/TestingData.shape[0])*100\n",
    "        \n",
    "    return Accurarcy,confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function of structurally call the functions:  Part A B C D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #test 0 represents testing data\n",
    "    #test 1 represents training data\n",
    "    case = 2\n",
    "    startTime = time.time()\n",
    "    TrainingDataFrame = inputTraining(case)\n",
    "    TrainingData = TrainingDataFrame.to_numpy()\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    #creating pickle of the clean data:\n",
    "    import pickle\n",
    "    with open(\"cleanData\",'wb') as f:\n",
    "        pickle.dump(TrainingDataFrame, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    (vocab_class0,vocab_class4, TotalVocab,wc) = vocabulary(TrainingData)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "    TestingData = inputTesting()\n",
    "    #TESTING DATA\n",
    "    (AccurarcyTest,confusionMatrixTest) = testing(TestingData,theta,phi,case,0)\n",
    "    print(\"------------------------------PART 1-A-------------------\")\n",
    "    print(\"Accuracy over Testing data is : \",AccurarcyTest,\"%\")\n",
    "    #TRAINING DATA\n",
    "    (AccurarcyTrain ,confusionMatrixTrain)= testing(TrainingData,theta,phi,case,1)\n",
    "    print(\"Accuracy over Training data is : \",AccurarcyTrain,\"%\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    endTime = time.time()-startTime\n",
    "    print(\"-----------------------TIME TAKEN------------------------\")\n",
    "    print(\"Total time taken is :\",endTime)\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    print(\"------------------------------PART 1-C-------------------\")\n",
    "    print(\"CONFUSION MATRIX FOR TEST DATA: \")\n",
    "    print(\" Actual \\t\\t Class 0 \\t Class 1\")\n",
    "    print(\"predicted Class 0 \\t\",confusionMatrixTest[0,0],\"\\t\\t\",confusionMatrixTest[0,1])\n",
    "    print(\"predicted Class 1 \\t\",confusionMatrixTest[1,0],\"\\t\\t\",confusionMatrixTest[1,1])\n",
    "    print(\"\\n\")\n",
    "#     print(\"CONFUSION MATRIX FOR TRAIN DATA: \")\n",
    "#     print(\" Actual \\t\\t Class 0 \\t Class 1\")\n",
    "#     print(\"predicted Class 0 \\t\",confusionMatrixTrain[0,0],\"\\t\\t\",confusionMatrixTrain[0,1])\n",
    "#     print(\"predicted Class 1 \\t\",confusionMatrixTrain[1,0],\"\\t\\t\",confusionMatrixTrain[1,1])\n",
    "#     print(confusionMatrixTest)\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 322119)\n"
     ]
    }
   ],
   "source": [
    "with open('cleanData','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(data['B'])\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi #charitytuesday\n",
      "  (0, 140104)\t0.39456621190909763\n",
      "  (0, 81596)\t0.9188675118969574\n"
     ]
    }
   ],
   "source": [
    "print(data['B'][1599999])\n",
    "print(tfidf[1599999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 50/1600 [03:55<2:06:05,  4.88s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-108b39d7fc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1600000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf_pf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# clf_pf = GaussianNB()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[1;32m    302\u001b[0m         return self._partial_fit(X, y, classes, _refit=False,\n\u001b[0;32m--> 303\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     def _partial_fit(self, X, y, classes=None, _refit=False,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# false positives from overflow in sum method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n\u001b[0m\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_pf = GaussianNB()\n",
    "\n",
    "for i in tqdm(range(0,1600000,1000)):\n",
    "    clf_pf.partial_fit(tfidf[i:i+1000].toarray(), data['A'][i:i+1000], np.array([0,4]))\n",
    "    \n",
    "# clf_pf = GaussianNB()\n",
    "# clf_pf.partial_fit(X, Y, np.unique(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
