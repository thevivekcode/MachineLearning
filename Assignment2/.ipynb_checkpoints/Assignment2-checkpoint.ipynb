{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_stemming(df):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Split the sentences to lists of words.\n",
    "    df['split'] = df['B'].str.strip().str.lower().str.split()\n",
    "    df = df.drop(columns=['B']) # Get rid of the old column.\n",
    "        \n",
    "    df['stopped_stemmed'] = df['split'].apply(lambda x: [stemmer.stem(item) for item in x if item not in stops and not item.startswith('@')])\n",
    "    df = df.drop(columns=['split']) # Get rid of the old column.\n",
    "    \n",
    "    df['B'] =  df['stopped_stemmed'].apply(lambda x: \" \".join(x))\n",
    "    df = df.drop(columns=['stopped_stemmed']) # Get rid of the old column.\n",
    "    print(df.columns)\n",
    "    return  df.to_numpy()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTraining(case):\n",
    "    df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    if case == 1:\n",
    "        return df.to_numpy()\n",
    "    elif case == 2:\n",
    "        return stopword_stemming(df)\n",
    "    \n",
    "\n",
    "def inputTesting():\n",
    "    df = pd.read_csv(\"./trainingandtestdata/testdata.manual.2009.06.14.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    data = df.to_numpy()\n",
    "    data = data[np.where(data[:,0]!=2)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = Polarity\n",
    "# B = TweetID\n",
    "# C = date of the tweet \n",
    "# D = query (lyx)\n",
    "# E = user\n",
    "# F = tweet\n",
    "\n",
    "#-----------CLASS------------\n",
    "\n",
    "#class 0 Positive \n",
    "#calss 4 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary of all words in training data\n",
    "\n",
    "def vocabulary(data):\n",
    "    data_class0 = data[np.where(data[:,0]==0)]\n",
    "#     data_class2 = data[np.where(data[:,0]==2)]\n",
    "    data_class4 = data[np.where(data[:,0]==4)]\n",
    "    dataClassList  = [data_class0,data_class4]\n",
    "    vocab_class0 = dict()\n",
    "    vocab_class2 = dict()\n",
    "    vocab_class4 = dict()\n",
    "    TotalVocab = dict()\n",
    "    wc = []\n",
    "    vocabClassList = [vocab_class0,vocab_class4]\n",
    "    for i,j in zip(dataClassList,vocabClassList):\n",
    "        totalWordCountInClass = 0\n",
    "        tweetList = i[:,1]\n",
    "        for tweet in tweetList:\n",
    "            tweetlist = tweet.replace(',',\" \").replace('.',\" \").split()\n",
    "#             tweetlist = tweet.split()\n",
    "            for words in tweetlist:\n",
    "                totalWordCountInClass+=1\n",
    "                if words in j:\n",
    "                    j[words]+=1\n",
    "                    TotalVocab[words]+=1\n",
    "                else:\n",
    "                    j[words]=1\n",
    "                    TotalVocab[words]=1\n",
    "        wc.append(totalWordCountInClass)\n",
    "    return vocab_class0,vocab_class4, TotalVocab,wc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding parameters\n",
    "\n",
    "def FindPhi(data):\n",
    "    phi = np.zeros((2,1))\n",
    "    phi[0,0] = (np.count_nonzero(data[:,0] == 0)+1)/(data.shape[0]+2)\n",
    "#     phi[1,0] = (np.count_nonzero(data[:,0] == 2)+1)/(data.shape[0]+3)\n",
    "    phi[1,0] = (np.count_nonzero(data[:,0] == 4)+1)/(data.shape[0]+2)\n",
    "    return phi\n",
    "\n",
    "def Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc):\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    theta = dict()\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    for words in TotalVocab:\n",
    "        theta[words] = []\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class0:\n",
    "                theta_ofThe_Word = (vocab_class0[words] + 1)/(wc[0] + len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[0] + len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "   \n",
    "#     for words in TotalVocab:\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class4:\n",
    "                theta_ofThe_Word = (vocab_class4[words] + 1)/(wc[1]+ len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[1]+ len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(TestingData,theta,phi):\n",
    "    prediction = []\n",
    "    for tweet in TestingData[:,1]:\n",
    "        class0 = phi[0,0]\n",
    "        class4 = phi[1,0]\n",
    "        tweetlist = tweet.replace(',',\" \").replace('.',\" \").split()\n",
    "#         tweetlist = tweet.split()\n",
    "        for words in tweetlist:\n",
    "            if words in theta:\n",
    "                class0 += math.log(theta[words][0]) \n",
    "            else:\n",
    "                class0 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "            \n",
    "            if words in theta:\n",
    "                class4 += math.log(theta[words][1]) \n",
    "            else :\n",
    "                class4 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "        \n",
    "        class0 += math.log(phi[0,0])\n",
    "        class4 += math.log(phi[1,0])\n",
    "        if class0 > class4:\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(4)\n",
    "    count =0\n",
    "    correct_class0 = 0\n",
    "    correct_class4 = 0\n",
    "    incorrect_class0 = 0\n",
    "    incorrect_class4 = 0\n",
    "    for i in  range(TestingData.shape[0]):\n",
    "        if prediction[i] == TestingData[i,0]:\n",
    "            if prediction[i] == 0:\n",
    "                correct_class0+=1\n",
    "            else:\n",
    "                correct_class4+=1\n",
    "            count+=1\n",
    "        else:\n",
    "            if prediction[i]==0:\n",
    "                incorrect_class4+=1\n",
    "            else:\n",
    "                incorrect_class0+=1\n",
    "    \n",
    "    confusionMatrix = np.array([[correct_class0,incorrect_class4],[incorrect_class0,correct_class4]])\n",
    "    Accurarcy = (count/TestingData.shape[0])*100\n",
    "        \n",
    "    return Accurarcy,confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    case = 1\n",
    "    startTime = time.time()\n",
    "    TrainingData = inputTraining(2)\n",
    "    (vocab_class0,vocab_class4, TotalVocab,wc) = vocabulary(TrainingData)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "#     print(len(theta)) #count no of words in vocabulary\n",
    "    TestingData = inputTesting()\n",
    "    (AccurarcyTest,confusionMatrixTest) = testing(TestingData,theta,phi)\n",
    "    print(\"------------------------------PART 1-A-------------------\")\n",
    "    print(\"Accuracy over Testing data is : \",AccurarcyTest,\"%\")\n",
    "    (AccurarcyTrain ,confusionMatrixTrain)= testing(TrainingData,theta,phi)\n",
    "    print(\"Accuracy over Training data is : \",AccurarcyTrain,\"%\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    endTime = time.time()-startTime\n",
    "    print(\"-----------------------TIME TAKEN------------------------\")\n",
    "    print(\"Total time taken is :\",endTime)\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    print(\"------------------------------PART 1-C-------------------\")\n",
    "    print(\"CONFUSION MATRIX\")\n",
    "    print(confusionMatrixTest)\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['A', 'B'], dtype='object')\n",
      "------------------------------PART 1-A-------------------\n",
      "Accuracy over Testing data is :  78.55153203342618 %\n",
      "Accuracy over Training data is :  80.99275 %\n",
      "---------------------------------------------------------\n",
      "\n",
      "-----------------------TIME TAKEN------------------------\n",
      "Total time taken is : 197.38640069961548\n",
      "---------------------------------------------------------\n",
      "\n",
      "------------------------------PART 1-C-------------------\n",
      "CONFUSION MATRIX\n",
      "[[126  26]\n",
      " [ 51 156]]\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "# s= time.time()\n",
    "# a=inputTraining(2)\n",
    "# print(time.time() -s)\n",
    "# a\n",
    "# for index,value in a.itterrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
