{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Stopping and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_stemming(df):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Split the sentences to lists of words.\n",
    "    df['split'] = df['B'].str.strip().str.lower().str.replace(\",\",\" \").str.replace(\".\",\" \").str.replace(\"!\",\"\").str.split()\n",
    "    df = df.drop(columns=['B']) # Get rid of the old column.\n",
    "        \n",
    "    df['stopped_stemmed'] = df['split'].apply(lambda x: [stemmer.stem(item) for item in x if item not in stops and not item.startswith('@')])\n",
    "    df = df.drop(columns=['split']) # Get rid of the old column.\n",
    "    \n",
    "    df['B'] =  df['stopped_stemmed'].apply(lambda x: \" \".join(x))\n",
    "    df = df.drop(columns=['stopped_stemmed']) # Get rid of the old column.\n",
    "#     return  df.to_numpy()\n",
    "    return df \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input: training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTraining(case):\n",
    "    df = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    if case == 1:\n",
    "        return df\n",
    "    elif case == 2:\n",
    "        return stopword_stemming(df)\n",
    "    \n",
    "\n",
    "def inputTesting():\n",
    "    df = pd.read_csv(\"./trainingandtestdata/testdata.manual.2009.06.14.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    data = df.to_numpy()\n",
    "    data = data[np.where(data[:,0]!=2)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = Polarity\n",
    "# B = TweetID\n",
    "# C = date of the tweet \n",
    "# D = query (lyx)\n",
    "# E = user\n",
    "# F = tweet\n",
    "\n",
    "#-----------CLASS------------\n",
    "\n",
    "#class 0 Positive \n",
    "#calss 4 negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary of all words in training data\n",
    "\n",
    "def vocabulary(data):\n",
    "    data_class0 = data[np.where(data[:,0]==0)]\n",
    "#     data_class2 = data[np.where(data[:,0]==2)]\n",
    "    data_class4 = data[np.where(data[:,0]==4)]\n",
    "    dataClassList  = [data_class0,data_class4]\n",
    "    vocab_class0 = dict()\n",
    "    vocab_class2 = dict()\n",
    "    vocab_class4 = dict()\n",
    "    TotalVocab = dict()\n",
    "    wc = []\n",
    "    vocabClassList = [vocab_class0,vocab_class4]\n",
    "    for i,j in zip(dataClassList,vocabClassList):\n",
    "        totalWordCountInClass = 0\n",
    "        tweetList = i[:,1]\n",
    "        for tweet in tweetList:\n",
    "            tweetlist = tweet.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "#             tweetlist = tweet.split()\n",
    "            for words in tweetlist:\n",
    "                totalWordCountInClass+=1\n",
    "                if words in j:\n",
    "                    j[words]+=1\n",
    "                    TotalVocab[words]+=1\n",
    "                else:\n",
    "                    j[words]=1\n",
    "                    TotalVocab[words]=1\n",
    "        wc.append(totalWordCountInClass)\n",
    "    print(\"Vocalbulary done\")\n",
    "    return vocab_class0,vocab_class4, TotalVocab,wc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the paarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding parameters\n",
    "\n",
    "def FindPhi(data):\n",
    "    phi = np.zeros((2,1))\n",
    "    phi[0,0] = (np.count_nonzero(data[:,0] == 0)+1)/(data.shape[0]+2)\n",
    "#     phi[1,0] = (np.count_nonzero(data[:,0] == 2)+1)/(data.shape[0]+3)\n",
    "    phi[1,0] = (np.count_nonzero(data[:,0] == 4)+1)/(data.shape[0]+2)\n",
    "    return phi\n",
    "\n",
    "def Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc):\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    theta = dict()\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    for words in TotalVocab:\n",
    "        theta[words] = []\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class0:\n",
    "                theta_ofThe_Word = (vocab_class0[words] + 1)/(wc[0] + len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[0] + len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "   \n",
    "#     for words in TotalVocab:\n",
    "        if(True):\n",
    "            theta_ofThe_Word = 0\n",
    "            if words in vocab_class4:\n",
    "                theta_ofThe_Word = (vocab_class4[words] + 1)/(wc[1]+ len(TotalVocab))\n",
    "            else:\n",
    "                theta_ofThe_Word = 1/(wc[1]+ len(TotalVocab))\n",
    "            theta[words].append(theta_ofThe_Word)\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    print(\"parameters calculated\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(TestingData,theta,phi,case,test):\n",
    "    prediction = []\n",
    "    for tweet in TestingData[:,1]:\n",
    "        class0 = phi[0,0]\n",
    "        class4 = phi[1,0]\n",
    "        \n",
    "        #TESTING DATA\n",
    "        if test == 0:\n",
    "            if case ==1:\n",
    "                \n",
    "                tweetlist = tweet.replace(',',\" \").replace('.',\" \").split()\n",
    "            \n",
    "            elif case ==2:\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "                tweetlist =  [stemmer.stem(item) for item in tweet.strip().lower().replace(\"!\",\"\").replace(\",\",\" \").replace(\".\",\" \").split() if item not in stops and not item.startswith('@')]\n",
    "       \n",
    "        # TRAINING DATA\n",
    "        if test == 1:\n",
    "            tweetlist = tweet.split()\n",
    "         \n",
    "        #CALCULATE PROBABILTY\n",
    "        for words in tweetlist:\n",
    "            if words in theta:\n",
    "                class0 += math.log(theta[words][0]) \n",
    "            else:\n",
    "                class0 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "            \n",
    "            if words in theta:\n",
    "                class4 += math.log(theta[words][1]) \n",
    "            else :\n",
    "                class4 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "        \n",
    "        class0 += math.log(phi[0,0])\n",
    "        class4 += math.log(phi[1,0])\n",
    "        if class0 > class4:\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(4)\n",
    "    count =0\n",
    "    correct_class0 = 0\n",
    "    correct_class4 = 0\n",
    "    incorrect_class0 = 0\n",
    "    incorrect_class4 = 0\n",
    "    for i in  range(TestingData.shape[0]):\n",
    "        if prediction[i] == TestingData[i,0]:\n",
    "            if prediction[i] == 0:\n",
    "                correct_class0+=1\n",
    "            else:\n",
    "                correct_class4+=1\n",
    "            count+=1\n",
    "        else:\n",
    "            if prediction[i]==0:\n",
    "                incorrect_class4+=1\n",
    "            else:\n",
    "                incorrect_class0+=1\n",
    "    \n",
    "    confusionMatrix = np.array([[correct_class0,incorrect_class4],[incorrect_class0,correct_class4]])\n",
    "    Accurarcy = (count/TestingData.shape[0])*100\n",
    "        \n",
    "    return Accurarcy,confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomPrediction(size):\n",
    "    import random\n",
    "    targets =[0,4]\n",
    "    randomPrediction =[]\n",
    "    for i in range(size):\n",
    "        randomPrediction.append(random.choice(targets))\n",
    "    return randomPrediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function of structurally call the functions:  Part A B C D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #test 0 represents testing data\n",
    "    #test 1 represents training data\n",
    "    case = 1\n",
    "    part = \"\"\n",
    "    if case ==1:\n",
    "        part ='A'\n",
    "    else:\n",
    "        part ='D'\n",
    "        \n",
    "    startTime = time.time()\n",
    "    TrainingDataFrame = inputTraining(case)\n",
    "    TrainingData = TrainingDataFrame.to_numpy()\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    #creating pickle of the clean data for further use\n",
    "    if case ==2:\n",
    "        import pickle\n",
    "        with open(\"cleanData\",'wb') as f:\n",
    "            pickle.dump(TrainingDataFrame, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    (vocab_class0,vocab_class4, TotalVocab,wc) = vocabulary(TrainingData)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "    TestingData = inputTesting()\n",
    "    #TESTING DATA\n",
    "    (AccurarcyTest,confusionMatrixTest) = testing(TestingData,theta,phi,case,0)\n",
    "    \n",
    "    \n",
    "    print(\"------------------------------PART 1-{}-------------------\".format(part))\n",
    "    print(\"Accuracy over Testing data is : \",AccurarcyTest,\"%\")\n",
    "    #TRAINING DATA\n",
    "    (AccurarcyTrain ,confusionMatrixTrain)= testing(TrainingData,theta,phi,case,1)\n",
    "    print(\"Accuracy over Training data is : \",AccurarcyTrain,\"%\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    endTime = time.time()-startTime\n",
    "    print(\"-----------------------TIME TAKEN------------------------\")\n",
    "    print(\"Total time taken is :\",endTime)\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"-----------------------------PART 1-B---------------------\")\n",
    "    print(\"-------------------RANDOM PREDICTION FOR TEST-------------\")\n",
    "    randomPredict =randomPrediction(TestingData.shape[0])\n",
    "\n",
    "    match = 0\n",
    "    for i in range(TestingData.shape[0]):\n",
    "        if randomPredict[i] == TestingData[i,0]:\n",
    "            match +=1\n",
    "    randomAcc = (match*100)/TestingData.shape[0]\n",
    "    print(\"Random guessing accuracy is \",randomAcc)\n",
    "    print(\"\\n\")\n",
    "    print(\"-------------------MAJORITY PREDICTION--------------------\\n\")\n",
    "    print(\"Since both targets are equally distributed in training data \\n\")\n",
    "    count4 = np.count_nonzero(TestingData[:,0]==4)\n",
    "    count0 = TestingData.shape[0] - count4\n",
    "    A4 = (count4*100)/TestingData.shape[0]\n",
    "    A0 = (count0*100)/TestingData.shape[0]\n",
    "    print(\"Accurarcy after taking postive class as majority \",A0)\n",
    "    print(\"Accurarcy after taking negative class as majority \",A4)\n",
    "\n",
    "    print(\"----------------------------------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"------------------------------PART 1-C-------------------\")\n",
    "    print(\"CONFUSION MATRIX FOR TEST DATA: \")\n",
    "    print(\" Actual \\t\\t Class 0 \\t Class 1\")\n",
    "    print(\"predicted Class 0 \\t\",confusionMatrixTest[0,0],\"\\t\\t\",confusionMatrixTest[0,1])\n",
    "    print(\"predicted Class 1 \\t\",confusionMatrixTest[1,0],\"\\t\\t\",confusionMatrixTest[1,1])\n",
    "    print(\"\\n\")\n",
    "    print(\"CONFUSION MATRIX FOR TRAIN DATA: \")\n",
    "    print(\" Actual \\t\\t Class 0 \\t\\t Class 1\")\n",
    "    print(\"predicted Class 0 \\t\",confusionMatrixTrain[0,0],\"\\t\\t\",confusionMatrixTrain[0,1])\n",
    "    print(\"predicted Class 1 \\t\",confusionMatrixTrain[1,0],\"\\t\\t\\t\",confusionMatrixTrain[1,1])\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf():\n",
    "    print(\"-------------Question F --> PART A---------------------\\n\")\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import pickle\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open('cleanData','rb') as f:\n",
    "        data=pickle.load(f)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(data['B'])\n",
    "    print(\"--------------Shape of Train vectorized data------------\")\n",
    "    print(tfidf.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "        #----------TRAINING MODEL TAKES > 1 HOUR-------------------\n",
    "#     clf_pf = GaussianNB()\n",
    "\n",
    "#     for i in tqdm(range(0,1600000,1000)):\n",
    "#         clf_pf.partial_fit(tfidf[i:i+1000].toarray(), data['A'][i:i+1000].to_numpy(), np.array([0,4]))\n",
    "    #-----------------------------------------------------------\n",
    "\n",
    "    #---------STORING MODEL TO SAVE TIME------------------------\n",
    "#     with open(\"vivekModel\",'wb') as f:\n",
    "#         pickle.dump(clf_pf,f)\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # loading the model pickle\n",
    "    with open(\"vivekModel\",'rb') as f:\n",
    "         model = pickle.load(f)\n",
    "        \n",
    "        \n",
    "    TestingData = inputTesting()\n",
    "    Testdataset = pd.DataFrame({'A': TestingData[:, 0], 'B': TestingData[:, 1]})\n",
    "    CleanTestData=stopword_stemming(Testdataset)\n",
    "    tfidfTest = vectorizer.transform(CleanTestData['B'])\n",
    "    print(\"--------------Shape of Test vectorized data------------\")\n",
    "    print(tfidfTest.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    prediction = model.predict(tfidfTest.toarray())\n",
    "    # print(prediction)\n",
    "    \n",
    "    match = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == CleanTestData[\"A\"][i]:\n",
    "            match += 1\n",
    "    accurarcy = (match*100)/len(prediction)\n",
    "    print(\"Accrarcy is \",accurarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Question F --> PART A---------------------\n",
      "\n",
      "--------------Shape of Train vectorized data------------\n",
      "(1600000, 322119)\n",
      "--------------------------------------------------------\n",
      "\n",
      "--------------Shape of Test vectorized data------------\n",
      "(359, 322119)\n",
      "--------------------------------------------------------\n",
      "\n",
      "Accrarcy is  48.18941504178273\n"
     ]
    }
   ],
   "source": [
    "tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn SelectPercentile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectPercenttile():\n",
    "    print(\"-------------Question F --> PART B---------------------\\n\")\n",
    "    from sklearn.feature_selection import SelectPercentile, chi2, f_classif\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import pickle\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open('cleanData','rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_new = vectorizer.fit_transform(data['B'],data[\"A\"])\n",
    "    print(\"--------------Shape of Train vectorized data------------\")\n",
    "    print(X_new.shape)\n",
    "    perSel = SelectPercentile(f_classif,percentile=10)\n",
    "    newFeature = perSel.fit_transform(X_new,data['A'].tolist())\n",
    "    print(newFeature.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "#     clf_pf = GaussianNB()\n",
    "\n",
    "#     for i in tqdm(range(0,1600000,1000)):\n",
    "#         clf_pf.partial_fit(newFeature[i:i+1000].toarray(), data['A'][i:i+1000].to_numpy(), np.array([0,4]))\n",
    "\n",
    "    #---------STORING MODEL TO SAVE TIME------------------------\n",
    "#     with open(\"vivekModel2\",'wb') as f:\n",
    "#         pickle.dump(clf_pf,f)\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    with open(\"vivekModel2\",'rb') as f:\n",
    "        newmodel = pickle.load(f)\n",
    "    \n",
    "    TestingData = inputTesting()\n",
    "    Testdataset = pd.DataFrame({'A': TestingData[:, 0], 'B': TestingData[:, 1]})\n",
    "    CleanTestData=stopword_stemming(Testdataset)\n",
    "\n",
    "    tfidfTest = vectorizer.transform(CleanTestData['B'])\n",
    "    print(\"--------------Shape of Test vectorized data------------\")\n",
    "    print(tfidfTest.shape)\n",
    "    newFeatureTest = perSel.transform(tfidfTest)\n",
    "    print(newFeatureTest.shape)\n",
    "    prediction = newmodel.predict(newFeatureTest.toarray())\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    match = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == CleanTestData[\"A\"][i]:\n",
    "            match += 1\n",
    "    accurarcy = (match*100)/len(prediction)\n",
    "    print(\"Accrarcy is \",accurarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Question F --> PART B---------------------\n",
      "\n",
      "--------------Shape of Train vectorized data------------\n",
      "(1600000, 322119)\n",
      "(1600000, 32212)\n",
      "--------------------------------------------------------\n",
      "\n",
      "--------------Shape of Test vectorized data------------\n",
      "(359, 322119)\n",
      "(359, 32212)\n",
      "--------------------------------------------------------\n",
      "\n",
      "Accrarcy is  54.59610027855153\n"
     ]
    }
   ],
   "source": [
    "selectPercenttile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
