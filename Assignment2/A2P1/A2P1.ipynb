{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_classif\n",
    "from sklearn import metrics\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5cm>INPUT DATA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTraining():\n",
    "    trainingDataFrame = pd.read_csv(\"./trainingandtestdata/training.1600000.processed.noemoticon.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    return trainingDataFrame\n",
    "    \n",
    "\n",
    "def inputTesting():\n",
    "    df = pd.read_csv(\"./trainingandtestdata/testdata.manual.2009.06.14.csv\",encoding = \"latin-1\", header=None, usecols=[0,5],names=['A','B'],index_col=False)\n",
    "    data = df.to_numpy()\n",
    "    data = data[np.where(data[:,0]!=2)]\n",
    "    testingDataFrame = pd.DataFrame({'A': data[:, 0], 'B': data[:, 1]})\n",
    "    return testingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataFrame = inputTraining()\n",
    "testingDataFrame = inputTesting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =red size=5cm>DATA DESCRIPTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = Polarity\n",
    "# B = TweetID\n",
    "# C = date of the tweet \n",
    "# D = query (lyx)\n",
    "# E = user\n",
    "# F = tweet\n",
    "\n",
    "#-----------CLASS------------\n",
    "\n",
    "#class 0 Negative \n",
    "#calss 4 Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size=5cm>CREATING VOCABULARY</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vocabulary of all words in training data\n",
    "\n",
    "def vocabulary(data):\n",
    "    \"\"\"\n",
    "    Inputs data in numpy format\n",
    "    Col 0: Label\n",
    "    Col 1: Text\n",
    "    \"\"\"\n",
    "    data_class0 = data[np.where(data[:,0]==0)]\n",
    "#     data_class2 = data[np.where(data[:,0]==2)]\n",
    "    data_class4 = data[np.where(data[:,0]==4)]\n",
    "    dataClassList  = [data_class0,data_class4]\n",
    "    \n",
    "    vocab_class0 = dict()\n",
    "    vocab_class4 = dict()\n",
    "    \n",
    "    TotalVocab = dict()\n",
    "    wc = []\n",
    "    vocabClassList = [vocab_class0,vocab_class4]\n",
    "    for i,j in zip(dataClassList,vocabClassList):\n",
    "        totalWordCountInClass = 0\n",
    "        tweetList = i[:,1]\n",
    "        for tweet in tweetList:\n",
    "            tweetlist = tweet.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "#             tweetlist = tweet.split()\n",
    "            for words in tweetlist:\n",
    "                totalWordCountInClass+=1\n",
    "                if words in j:\n",
    "                    j[words]+=1\n",
    "                    TotalVocab[words]+=1\n",
    "                elif words in TotalVocab:\n",
    "                    TotalVocab[words]+=1\n",
    "                    j[words]=1\n",
    "                else:\n",
    "                    TotalVocab[words] = 1\n",
    "                    j[words] = 1\n",
    "                    \n",
    "               \n",
    "                    \n",
    "        wc.append(totalWordCountInClass)\n",
    "    print(\"Vocalbulary done\")\n",
    "    return vocab_class0,vocab_class4, TotalVocab,wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size=5cm>CALCULATING PARAMETERS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding parameters\n",
    "\n",
    "def FindPhi(data):\n",
    "    \"\"\"\n",
    "    Inputs data in numpy format\n",
    "    Col 0: Label\n",
    "    Col 1: Text\n",
    "    \"\"\"\n",
    "    phi = np.zeros((2,1))\n",
    "    phi[0,0] = (np.count_nonzero(data[:,0] == 0)+1)/(data.shape[0]+2)\n",
    "    phi[1,0] = (np.count_nonzero(data[:,0] == 4)+1)/(data.shape[0]+2)\n",
    "    return phi\n",
    "\n",
    "def Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc):\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    theta = dict()\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    for words in TotalVocab:\n",
    "        theta[words] = []\n",
    "\n",
    "        theta_ofThe_Word = 0\n",
    "        if words in vocab_class0:\n",
    "            theta_ofThe_Word = (vocab_class0[words] + 1)/(wc[0] + len(TotalVocab))\n",
    "        else:\n",
    "            theta_ofThe_Word = 1/(wc[0] + len(TotalVocab))\n",
    "        theta[words].append(theta_ofThe_Word)\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "        theta_ofThe_Word = 0\n",
    "        if words in vocab_class4:\n",
    "            theta_ofThe_Word = (vocab_class4[words] + 1)/(wc[1]+ len(TotalVocab))\n",
    "        else:\n",
    "            theta_ofThe_Word = 1/(wc[1]+ len(TotalVocab))\n",
    "        theta[words].append(theta_ofThe_Word)\n",
    "        \n",
    "    #----------------------------------------------------------------------------------------------\n",
    "    print(\"parameters calculated\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =red size=5cm>PREDICTING :</font>\n",
    "<font color = blue size=4cm>Accuracy, Confusion Matrix and Probablity </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predicting(TestingData,theta,phi):\n",
    "\n",
    "    prediction = []\n",
    "    postiveProbality = []\n",
    "    for tweet in TestingData[:,1]:\n",
    "        class0 = phi[0,0]\n",
    "        class4 = phi[1,0]\n",
    "        \n",
    "        tweetlist = tweet.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "        \n",
    "        #CALCULATE PROBABILTY\n",
    "        for words in tweetlist:\n",
    "            if words in theta:\n",
    "                class0 += math.log(theta[words][0]) \n",
    "            else:\n",
    "                class0 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "            \n",
    "            if words in theta:\n",
    "                class4 += math.log(theta[words][1]) \n",
    "            else :\n",
    "                class4 += math.log(1/(len(tweetlist) + len(theta)))\n",
    "        \n",
    "        class0 += math.log(phi[0,0])\n",
    "        class4 += math.log(phi[1,0])\n",
    "        postiveProbality.append(class4)\n",
    "        if class0 > class4:\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(4)\n",
    "            \n",
    "    count =0\n",
    "    correct_class0 = 0\n",
    "    correct_class4 = 0\n",
    "    incorrect_class0 = 0\n",
    "    incorrect_class4 = 0\n",
    "    for i in  range(TestingData.shape[0]):\n",
    "        if prediction[i] == TestingData[i,0]:\n",
    "            if prediction[i] == 0:\n",
    "                correct_class0+=1\n",
    "            else:\n",
    "                correct_class4+=1\n",
    "            count+=1\n",
    "        else:\n",
    "            if prediction[i]==0:\n",
    "                incorrect_class4+=1\n",
    "            else:\n",
    "                incorrect_class0+=1\n",
    "    \n",
    "    confusionMatrix = np.array([[correct_class0,incorrect_class4],[incorrect_class0,correct_class4]])\n",
    "    Accurarcy = (count/TestingData.shape[0])*100\n",
    "        \n",
    "    return Accurarcy,confusionMatrix,postiveProbality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1A(trainingDataFrame,testingDataFrame):\n",
    "    \n",
    "    #-------------------LEARNING-------------------------------------------------------\n",
    "    trainDataframe = trainingDataFrame.copy()\n",
    "    trainDataframe[\"B\"] =  trainDataframe[\"B\"].str.replace(\",\",\" \").str.replace(\".\",\" \")\n",
    "    TrainingData = trainDataframe.to_numpy()\n",
    "    (vocab_class0,vocab_class4, TotalVocab,wc) = vocabulary(TrainingData)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "    #----------------------------------------------------------------------------------\n",
    "    \n",
    "    #------------------TESTING---------------------------------------------------------\n",
    "    testDataFrame = testingDataFrame.copy()\n",
    "    testDataFrame[\"B\"] =  testDataFrame[\"B\"].str.replace(\",\",\" \").str.replace(\".\",\" \")\n",
    "    TestingData = testDataFrame.to_numpy()\n",
    "    Accurarcy, confusionMatrixTEST_1A, postiveProbalityTEST_1A = Predicting(TestingData,theta,phi)\n",
    "    print(\"Accurarcy over TESTING data is :\",Accurarcy,\" %\")\n",
    "    #----------------------------------------------------------------------------------\n",
    "    \n",
    "    #------------------TRAINING--------------------------------------------------------\n",
    "    Accurarcy, confusionMatrixTRAIN_1A, postiveProbalityTRAIN_1A = Predicting(TrainingData,theta,phi)\n",
    "    print(\"Accurarcy over TRAINING data is :\",Accurarcy,\" %\")\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    return confusionMatrixTEST_1A, postiveProbalityTEST_1A, postiveProbalityTRAIN_1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocalbulary done\n",
      "parameters calculated\n",
      "Accurarcy over TESTING data is : 80.77994428969359  %\n",
      "Accurarcy over TRAINING data is : 84.932875  %\n"
     ]
    }
   ],
   "source": [
    "confusionMatrixTEST_1A, postiveProbalityTEST_1A, postiveProbalityTRAIN_1A = part1A(trainingDataFrame,testingDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5cm>RANDOM PREDICTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomPrediction(size):\n",
    "    \"\"\"\n",
    "    Give a size,\n",
    "    returns random choices \n",
    "    given a target to choose from.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    targets =[0,4]\n",
    "    randomPrediction =[]\n",
    "    for i in range(size):\n",
    "        randomPrediction.append(random.choice(targets))\n",
    "    return randomPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1B(testingDataFrame):\n",
    "    TestingData  = testingDataFrame.to_numpy()\n",
    "    randomPredict =randomPrediction(TestingData.shape[0])\n",
    "    match = 0\n",
    "    for i in range(TestingData.shape[0]):\n",
    "        if randomPredict[i] == TestingData[i,0]:\n",
    "            match +=1\n",
    "    randomAcc = (match*100)/TestingData.shape[0]\n",
    "    print(\"Random guessing accuracy on test data is \",randomAcc,\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random guessing accuracy on test data is  51.532033426183844  %\n"
     ]
    }
   ],
   "source": [
    "part1B(testingDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(confusionMatrixTEST_1A):\n",
    "    df_cm = pd.DataFrame(confusionMatrixTEST_1A ,index = [i for i in [0,4]],\n",
    "                      columns = [i for i in [0,4]], dtype=int)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sn.heatmap(df_cm, annot=True,fmt='d')\n",
    "    plt.xlabel(\"ACTUAL VALUE\",fontsize = 15)\n",
    "    plt.ylabel(\"PREDICTED VALUE\",fontsize = 15)\n",
    "    plt.title(\"Confusion Matrix: TEST\",color =\"red\",size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrix(confusionMatrixTEST_1A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5cm>TOKENIZATION, STOPPING ADND STEMMING</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(df):\n",
    "    \"\"\"\n",
    "    Takes dataframes as input \n",
    "    and returns a cleaned dataframe.\n",
    "    \"\"\"\n",
    "    #----------Cleaning URLs ------------------------------------\n",
    "    def removeURL(text):\n",
    "        return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    df[\"B\"] = df[\"B\"].apply(lambda text: removeURL(text) )\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #-------------Cleaning punctuation---------------------------\n",
    "    def removePunctuation(text):    \n",
    "#         punctuation =  '!\"#$\\()*+/<=>?[\\\\]^_`{|}~'\n",
    "        punctuation =  '!\"#{}'\n",
    "        for c in punctuation:\n",
    "            text= text.replace(c,\"\")\n",
    "        return text\n",
    "    \n",
    "    df[\"B\"] = df[\"B\"].apply(lambda text: removePunctuation(text) )\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #-------------Tokenization------------------------------------\n",
    "    def tokenization(text):\n",
    "#         pointOfSplit = ',.:;'\n",
    "        pointOfSplit = ',.'\n",
    "        for c in pointOfSplit:\n",
    "            text= text.replace(c,\" \")\n",
    "        return text.lower().split()\n",
    "    \n",
    "    df[\"B\"] = df[\"B\"].apply(lambda text: tokenization(text) )\n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #-----------english stoppers and stemmer---------------------\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    df['B'] = df['B'].apply(lambda x: [stemmer.stem(item) for item in x if item not in stops and not item.startswith('@')])\n",
    "    \n",
    "    df['B'] =  df['B'].apply(lambda x: \" \".join(x))\n",
    "    #------------------------------------------------------------\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1D(trainingDataFrame,testingDataFrame):\n",
    "    s =time.time()\n",
    "    \n",
    "    trainDataframe = trainingDataFrame.copy()\n",
    "#     \n",
    "#     trainDataframe =  cleaningData(trainDataframe)  # CLEANING THE TRAINING DATA\n",
    "#     with open('cleanTrainData','wb') as f:\n",
    "#         pickle.dump(trainDataframe, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open('cleanTrainData','rb') as f:\n",
    "        trainDataframe = pickle.load(f)\n",
    "    TrainingData = trainDataframe.to_numpy()\n",
    "    \n",
    "    \n",
    "    #-------------------LEARNING-------------------------------------------------------\n",
    "    (vocab_class0, vocab_class4, TotalVocab,wc) = vocabulary(TrainingData)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "    #----------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #------------------TESTING---------------------------------------------------------\n",
    "    testDataFrame = testingDataFrame.copy()\n",
    "    testDataFrame =  cleaningData(testDataFrame) # CLEANING THE TESTING DATA\n",
    "    TestingData = testDataFrame.to_numpy()\n",
    "    Accurarcy, confusionMatrixTEST_1D, postiveProbalityTEST_1D = Predicting(TestingData,theta,phi)\n",
    "    print(\"Accurarcy over TESTING data is :\",Accurarcy,\" %\\n\")\n",
    "    #----------------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    print(\"TOTAL TIME :\",time.time()-s)\n",
    "    return postiveProbalityTEST_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocalbulary done\n",
      "parameters calculated\n",
      "Accurarcy over TESTING data is : 82.17270194986072  %\n",
      "\n",
      "TOTAL TIME : 8.547100067138672\n"
     ]
    }
   ],
   "source": [
    "postiveProbalityTEST_1D = part1D(trainingDataFrame,testingDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =red size = 5cm> FEATURE ENGINEERING </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramification(Data):\n",
    "    \"\"\"\n",
    "    Takes data as numpy ndarray\n",
    "    \"\"\"\n",
    "    data = Data.copy()\n",
    "    for index in range(data.shape[0]):\n",
    "        newData = []\n",
    "        rowList = data[index,1].split()\n",
    "        for i in range(len(rowList)-1):\n",
    "            newData.append(rowList[i])\n",
    "            newData.append(rowList[i]+rowList[i+1])\n",
    "        data[index,1] = \" \".join(newData.copy())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging():\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "\n",
    "    with open('cleanTrainData','rb') as f:\n",
    "        trainDataframe = pickle.load(f)\n",
    "\n",
    "    trainDataframe[\"B\"] = trainDataframe[\"B\"].str.split()\n",
    "    trainDataframe[\"B\"]=trainDataframe[\"B\"].apply(lambda wordsList:nltk.pos_tag(wordsList))\n",
    "    with open('POS_TaggedTrain','wb') as f:\n",
    "        pickle.dump(trainDataframe,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Data():\n",
    "    with open('cleanTrainData','rb') as f:\n",
    "        trainDataframe = pickle.load(f)\n",
    "        \n",
    "    with open('POS_TaggedTrain','rb') as f:\n",
    "        tagged_trainDataframe = pickle.load(f)\n",
    "        \n",
    "    trainData = trainDataframe.to_numpy()\n",
    "#     POS_tagges = [\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\"]\n",
    "#     POS_tagges = [\"RB\",\"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\"]\n",
    "    POS_tagges = [\"RB\",\"RBR\",\"RBS\"]\n",
    "    text = []\n",
    "    for i in tqdm(range(tagged_trainDataframe.shape[0])):\n",
    "        tweetWordList = []\n",
    "        for j in tagged_trainDataframe[\"B\"][i]:\n",
    "            if j[1] in POS_tagges:\n",
    "                tweetWordList.append(j[0])\n",
    "                tweetWordList.append(j[0])\n",
    "                tweetWordList.append(j[0])\n",
    "            else:\n",
    "                tweetWordList.append(j[0])\n",
    "        trainData[i,1] = \" \".join(tweetWordList)\n",
    "                \n",
    "    dataset = pd.DataFrame({'A': trainData[:, 0], 'B': trainData[:, 1]})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering():\n",
    "\n",
    "    \n",
    "    #-----POS-----------------------------------\n",
    "    trainDataframe = POS_Data()\n",
    "    #-------------------------------------------\n",
    "    \n",
    "    testingDataFrame = inputTesting()\n",
    "    \n",
    "    TrainingData = trainDataframe.to_numpy()\n",
    "    \n",
    "    testDataFrame =  cleaningData(testingDataFrame)\n",
    "    TestingData = testDataFrame.to_numpy()\n",
    "    \n",
    "    #------ADDING FEATURES: BIGRAM-----------------\n",
    "    TrainingDataBG =  bigramification(TrainingData)\n",
    "    TestingDataBG =  bigramification(TestingData)\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    \n",
    "    #------------------LEARNING-------------------------------\n",
    "    (vocab_class0, vocab_class4, TotalVocab,wc) = vocabulary(TrainingDataBG)\n",
    "    phi = FindPhi(TrainingData)\n",
    "    theta  = Findtheta(vocab_class0,  vocab_class4, TotalVocab,wc)\n",
    "    #---------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #---------------------TESTING---------------------------------------\n",
    "    Accurarcy, confusionMatrixTEST_1E, postiveProbalityTEST_1E = Predicting(TestingData,theta,phi)\n",
    "    print(\"Accurarcy over TESTING data is :\",Accurarcy,\" %\")\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    return postiveProbalityTEST_1E\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:58<00:00, 27156.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocalbulary done\n",
      "parameters calculated\n",
      "Accurarcy over TESTING data is : 83.56545961002786  %\n"
     ]
    }
   ],
   "source": [
    "postiveProbalityTEST_1E = featureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size=5cm>TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(trainingDataFrame,testingDataFrame):\n",
    "    with open('cleanTrainData','rb') as f:\n",
    "        trainDataframe = pickle.load(f)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(min_df=0.0007)\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    X_tfidf = vectorizer.fit_transform(trainDataframe['B'])\n",
    "    \n",
    "    print(\"--------------Shape of Train vectorized data------------\")\n",
    "    print(X_tfidf.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    for i in tqdm(range(0,trainDataframe.shape[0],1000)):\n",
    "        model.partial_fit(X_tfidf[i:i+1000].toarray(), trainDataframe['A'][i:i+1000].to_numpy(), np.array([0,4]))\n",
    "        \n",
    "    \n",
    "    testDataFrame = testingDataFrame.copy()\n",
    "    testDataFrame =  cleaningData(testDataFrame) # CLEANING THE TESTING DATA\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(testDataFrame['B'])\n",
    "    \n",
    "    print(\"--------------Shape of Test vectorized data------------\")\n",
    "    print(X_test_tfidf.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    prediction = model.predict(X_test_tfidf.toarray())\n",
    "    tfidf_Prob = model.predict_log_proba(X_test_tfidf.toarray())[:,1]\n",
    "    \n",
    "    match = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == testDataFrame[\"A\"][i]:\n",
    "            match += 1\n",
    "    accurarcy = (match*100)/len(prediction)\n",
    "    print(\"Accrarcy is \",accurarcy)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    return tfidf_Prob\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1600 [00:00<00:30, 52.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Shape of Train vectorized data------------\n",
      "(1600000, 1341)\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:25<00:00, 63.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Shape of Test vectorized data------------\n",
      "(359, 1341)\n",
      "--------------------------------------------------------\n",
      "\n",
      "Accrarcy is  81.05849582172702\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_Prob = tfidf(trainingDataFrame,testingDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size=5cm>SKLearn SelectPercentile </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectPercentile(trainingDataFrame,testingDataFrame):\n",
    "    \n",
    "    with open('cleanTrainData','rb') as f:\n",
    "        trainDataframe = pickle.load(f)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(min_df=0.0007)\n",
    "    \n",
    "    X_tfidf = vectorizer.fit_transform(trainDataframe['B'])\n",
    "    \n",
    "    selPer = SelectPercentile(f_classif,percentile=20)\n",
    "    X_tfidf_new = selPer.fit_transform(X_tfidf,trainDataframe['A'].tolist())\n",
    "\n",
    "    \n",
    "    print(\"--------------Shape of Train vectorized data------------\")\n",
    "    print(X_tfidf.shape)\n",
    "    print(X_tfidf_new.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    for i in tqdm(range(0,trainDataframe.shape[0],1000)):\n",
    "        model.partial_fit(X_tfidf_new[i:i+1000].toarray(), trainDataframe['A'][i:i+1000].to_numpy(), np.array([0,4]))\n",
    "        \n",
    "        \n",
    "    testDataFrame = testingDataFrame.copy()\n",
    "    testDataFrame =  cleaningData(testDataFrame) # CLEANING THE TESTING DATA\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(testDataFrame['B'])\n",
    "    \n",
    "    X_test_tfidf_new = selPer.transform(X_test_tfidf)\n",
    "    \n",
    "    print(\"--------------Shape of Test vectorized data------------\")\n",
    "    print(X_test_tfidf.shape)\n",
    "    print(X_test_tfidf_new.shape)\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    prediction = model.predict(X_test_tfidf_new.toarray())\n",
    "    selPer_Prob = model.predict_log_proba(X_test_tfidf_new.toarray())[:,1]\n",
    "    \n",
    "    match = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == testDataFrame[\"A\"][i]:\n",
    "            match += 1\n",
    "    accurarcy = (match*100)/len(prediction)\n",
    "    print(\"Accrarcy is \",accurarcy)\n",
    "    return selPer_Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 23/1600 [00:00<00:06, 229.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Shape of Train vectorized data------------\n",
      "(1600000, 1341)\n",
      "(1600000, 268)\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:06<00:00, 257.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Shape of Test vectorized data------------\n",
      "(359, 1341)\n",
      "(359, 268)\n",
      "--------------------------------------------------------\n",
      "\n",
      "Accrarcy is  76.32311977715878\n"
     ]
    }
   ],
   "source": [
    "selPer_Prob = selectPercentile(trainingDataFrame,testingDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1-G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size = 5cm> ROC : RECEIVER OPERATING CHARACTERSTIC </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y,prob_scores,label,title):\n",
    "    (fpr, tpr, thresholds) = metrics.roc_curve(y, prob_scores, pos_label=label)\n",
    "    auc = metrics.roc_auc_score(y, prob_scores)\n",
    "    p, = plt.plot(fpr,tpr,label=\"area under curve is \"+str(auc),color ='green')\n",
    "    plt.xlabel(\"False Positive Rate\",color='red')\n",
    "    plt.ylabel(\"True Positive Rate\",color='red')\n",
    "    plt.title(title,color ='blue',fontsize = 15)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = ROC(testingDataFrame['A'].tolist(),postiveProbalityTEST_1A,4,\"PART A: UNCLEAND DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = ROC(testingDataFrame['A'].tolist(),postiveProbalityTEST_1D,4,\"PART D: CLEANED DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = ROC(testingDataFrame['A'].tolist(),postiveProbalityTEST_1E,4,\"PART E: Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = ROC(testingDataFrame['A'].tolist(),tfidf_Prob,4,\"PART F: TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = ROC(testingDataFrame['A'].tolist(),selPer_Prob,4,\"PART F: SELECT-PERCENTILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
